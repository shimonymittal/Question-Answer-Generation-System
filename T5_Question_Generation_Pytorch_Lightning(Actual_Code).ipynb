{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jMeOslZ9Qxlx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f4c8025-dff0-4a65-d246-fbd270c28e4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.3/519.3 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m89.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m69.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m722.8/722.8 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.6/731.6 kB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install --quiet  datasets #to access race dataset\n",
        "!pip install --quiet pyarrow   #to deal with parquet files for saving dataset if required\n",
        "!pip install --quiet  tqdm     #for progress bars\n",
        "!pip install --quiet transformers # for t5 model\n",
        "!pip install --quiet tokenizers  #tokenizers from HuggingFace\n",
        "!pip install --quiet sentencepiece #subword tokenizer used by T5\n",
        "!pip install --quiet pytorch-lightning # pytorch wrapper\n",
        "!pip install --quiet torchtext # text utilities"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhbOe_M7omZN",
        "outputId": "3d23f503-7f6a-4961-c3b2-c4fd55094ba8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnkwYBQ6TWB4"
      },
      "source": [
        "# Fetching Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QnjddTNlS6Dz"
      },
      "outputs": [],
      "source": [
        "#imports\n",
        "import pandas as pd\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from pprint import pprint\n",
        "import copy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fetching Custom Dataset"
      ],
      "metadata": {
        "id": "l_bE1EaCaA32"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import io\n",
        "import pandas as pd\n",
        "uploaded = files.upload()\n",
        "df2 = pd.read_csv(io.BytesIO(uploaded['Dataset - Sheet1.csv']))"
      ],
      "metadata": {
        "id": "ZUpgJTAcTZHP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "outputId": "1e00ad03-6937-41ea-d9de-ae45e2a16275"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-261267ca-a0be-4d6a-85fc-d13528f8171c\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-261267ca-a0be-4d6a-85fc-d13528f8171c\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-925819e9bbb4>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0muploaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdf2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muploaded\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Dataset - Sheet1.csv'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m   \"\"\"\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m   \u001b[0muploaded_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_upload_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultiple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m   \u001b[0;31m# Mapping from original filename to filename as saved locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m   \u001b[0mlocal_filenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36m_upload_files\u001b[0;34m(multiple)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m   \u001b[0;31m# First result is always an indication that the file picker has completed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m   result = _output.eval_js(\n\u001b[0m\u001b[1;32m    157\u001b[0m       'google.colab._files._uploadFiles(\"{input_id}\", \"{output_id}\")'.format(\n\u001b[1;32m    158\u001b[0m           \u001b[0minput_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df3=df2.iloc[0:177, 1:4]\n",
        "df3"
      ],
      "metadata": {
        "id": "Ft2VLImaVCGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataset(filename,  verbose = False):\n",
        "  data  = pd.read_csv(filename, skipinitialspace=True, usecols=['Context','Question','Answer'] )\n",
        "  result_df1  = pd.DataFrame(columns = ['context', 'question','answer'])\n",
        "  result_df1 = data\n",
        "  return result_df1"
      ],
      "metadata": {
        "id": "dUeWslZzV4WK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train, test = train_test_split(df2, test_size=0.2)"
      ],
      "metadata": {
        "id": "Naf34hAqV4jr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train1 , df_validation1 = create_pandas_dataset1(train) , create_pandas_dataset1(valid)\n",
        "print(f\"\\n Total Train Samples:{df_train1.shape} , Total Validation Samples:{df_validation1.shape}\")"
      ],
      "metadata": {
        "id": "3uzYSdXOboDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "INC4ECm5AFip"
      },
      "outputs": [],
      "source": [
        "device  = 'cuda' if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_pandas_dataset1(data,\n",
        "                          answer_threshold=512,\n",
        "                          verbose = False):\n",
        "\n",
        "  ''' Create a Pandas Dataframe from hugging face dataset.\n",
        "  Params:\n",
        "        answer_threshold: Only consider those Question Answer pairs where the Answer is short.\n",
        "  '''\n",
        "  #print(data)\n",
        "  count_long ,count_short = 0 , 0\n",
        "  result_df  = pd.DataFrame(columns = ['context', 'question','answer'])\n",
        "  for val in enumerate(tqdm(data)):\n",
        "      passage = val['Context']\n",
        "      question = val['Question']\n",
        "      answer = val['Answer']\n",
        "      #answer = val['options'][ord(ans)-65]\n",
        "      no_of_words = len(answer.split())\n",
        "      if no_of_words >= answer_threshold:\n",
        "          count_long = count_long + 1\n",
        "          continue\n",
        "      else:\n",
        "          result_df.loc[count_short] = [passage] +[question] + [answer]\n",
        "          count_short = count_short + 1\n",
        "  if verbose:\n",
        "    return (result_df,\n",
        "            count_long,\n",
        "            count_short)\n",
        "  else:\n",
        "    return result_df"
      ],
      "metadata": {
        "id": "DOKhkYV0V5R7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "snPR6qxi-PxA"
      },
      "outputs": [],
      "source": [
        "pd.options.display.max_rows , pd.options.display.max_columns  = 100,100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cRRLG8gZ1IcN"
      },
      "outputs": [],
      "source": [
        "def create_pandas_dataset(data,\n",
        "                          answer_threshold=512,\n",
        "                          verbose = False):\n",
        "\n",
        "  ''' Create a Pandas Dataframe from hugging face dataset.\n",
        "  Params:\n",
        "        answer_threshold: Only consider those Question Answer pairs where the Answer is short.\n",
        "  '''\n",
        "  count_long ,count_short = 0 , 0\n",
        "  result_df  = pd.DataFrame(columns = ['context', 'question','answer'])\n",
        "  for index,val in enumerate(tqdm(data)):\n",
        "      passage = val['article']\n",
        "      question = val['question']\n",
        "      ans = val['answer']\n",
        "      answer = val['options'][ord(ans)-65]\n",
        "      no_of_words = len(answer.split())\n",
        "      if no_of_words >= answer_threshold:\n",
        "          count_long = count_long + 1\n",
        "          continue\n",
        "      else:\n",
        "          result_df.loc[count_short] = [passage] +[question] + [answer]\n",
        "          count_short = count_short + 1\n",
        "  if verbose:\n",
        "    return (result_df,\n",
        "            count_long,\n",
        "            count_short)\n",
        "  else:\n",
        "    return result_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqctRHUeLk0M"
      },
      "source": [
        "#Changed it to Race Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ra4F3pizTZ-9"
      },
      "outputs": [],
      "source": [
        "train_dataset = load_dataset('race','middle', split='train')\n",
        "valid_dataset = load_dataset('race', 'middle', split='validation')\n",
        "print(f\"Total Train Samples:{len(train_dataset)} , Total Validation Samples:{len(valid_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LD7O9vasveLR"
      },
      "outputs": [],
      "source": [
        "print(train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JGH_ObQA0j3W"
      },
      "outputs": [],
      "source": [
        "df_train , df_validation = create_pandas_dataset(train_dataset) , create_pandas_dataset(valid_dataset)\n",
        "print(f\"\\n Total Train Samples:{df_train.shape} , Total Validation Samples:{df_validation.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49j_Kx6A5cb0"
      },
      "outputs": [],
      "source": [
        "# Saving data for future use\n",
        "df_train.to_parquet('train_race.parquet')\n",
        "df_validation.to_parquet('validation_race.parquet')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run this one"
      ],
      "metadata": {
        "id": "B4_apmqJgwB-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yrdjFLeKHNpY"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "pd.read_parquet('/content/drive/MyDrive/train_race.parquet', engine='pyarrow')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rnhad7usfJiK"
      },
      "source": [
        "# Creating a Pytorch DataSet for T5 Training and Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7y_Ver9Eg26r"
      },
      "outputs": [],
      "source": [
        "from transformers import (\n",
        "    AdamW,\n",
        "    T5ForConditionalGeneration,\n",
        "    T5Tokenizer,\n",
        "    get_linear_schedule_with_warmup\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7JNUExtpgrNS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214,
          "referenced_widgets": [
            "d5e11c3b5fff46eeaf2c85993ace75b2",
            "74a8a44aedf343f2a98fdd00757853d7",
            "c19ac1653d26498bbb9d6ef19c607f46",
            "83cf44b7e2794d36aac3cf91eb2b0b0a",
            "201709eb32744efea1a008ffe586bdc2",
            "6168d397b8d8421eb4dde74d4cc0edfe",
            "2c8bb827a78d45ed94a1f4473dbfea25",
            "bf126876f4964cf4afb22b9b1761da37",
            "ee042054b6c640d792a9a41b78de2046",
            "2a446e8c9c954c94a626235260066dc2",
            "24f399cc816a4c2e950fc7412263cb12",
            "977c6b81dea447cba92c8447a8f9c145",
            "cf784c8b0e184a6d868ac799235f9912",
            "461f05816fb3479e95361b1ebf5070b9",
            "c5e5bfeae6d14e01a4d77f664e5acd2c",
            "96f60849927d481888a26b8a01c8da2e",
            "22ea660ee0ac4158844292a259c02dd7",
            "9c083418876b416095f9a1d6b35942a0",
            "94eb96b24434400e81ba9bb86fe48d7d",
            "eff6419b09aa489ebd3c9593338d1435",
            "da0f7b3bed744817971f085b939b79ee",
            "e298678b21144c35a26544d9e96bfd19",
            "5a7550bcc87746e08816d7eaf3bd041d",
            "31ad55285c0c4a7ca2c0a215a23fb63e",
            "0170d9b4cc4a4afbb6488b3502a24b31",
            "b9f6b58de4884fdf8cf7f2de033836af",
            "28e9f0a52db84aec8d67a75791e5a055",
            "315a6cdfcaa949f1830c6693811979e1",
            "5c014b3ef0df4738abd9f911fb26d4b0",
            "cf400b5c4d3049fea8a31d1beedbd2a9",
            "58e7fa13a64f457d87b3dc0b3dd65eb7",
            "d640382e5b2e4d7d8b13507286a1e969",
            "527b3e74dc5c45cbb162097d359f6c42",
            "d0497d0eb7154813a42de4d980369ef0",
            "198ab701b24542c68001a66196749e4c",
            "eed1f48b63ff4c71a4dbdebf6c06cfca",
            "3b3f1e1986404d52a493cdf8eb6bd013",
            "1b72316f5ca44e119d48a1041d1ec3e2",
            "30310898682a4be2ad229c97a3e6da33",
            "fe219ceea3994a2eae493aa50eb0c2ae",
            "d94ed8dadd4b4824a37b5565c7b4e82d",
            "24453c487d994db9a878f3be44eef935",
            "e31a1e2cd1ec4c35b04fb750bd9b9ea0",
            "068d7db575f246d1b2b611b63cb6aaf2",
            "41ad0b3d44154c8fb73e5618e20cd49a",
            "e4dfb0488eb44b8a8de06f2cb573a7ac",
            "e06ecac16c3c4ed19005199adc6e6900",
            "ff8001222b314654bb9f276de4386a80",
            "386b7b0c0b8a427c95b359552fe9db9e",
            "bc35f104d9da47ecb4165532c97785a9",
            "8e81cb8ffc9349c69dafcc15f5cd96a8",
            "79df26fa0fe247b896975d76988406dd",
            "80f0d71a22644d00a78a3a0e529835ec",
            "124ab81661594413a33461cc88d745e8",
            "b2c385ee06e54dca873dd64168cefffb"
          ]
        },
        "outputId": "0c7ea44a-982c-4bb2-b940-e7a21e83b796"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)ve/main/spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d5e11c3b5fff46eeaf2c85993ace75b2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "977c6b81dea447cba92c8447a8f9c145"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using the legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5a7550bcc87746e08816d7eaf3bd041d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d0497d0eb7154813a42de4d980369ef0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)neration_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "41ad0b3d44154c8fb73e5618e20cd49a"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "t5_tokenizer = T5Tokenizer.from_pretrained('t5-small',model_max_length=512)\n",
        "t5_model = T5ForConditionalGeneration.from_pretrained('t5-small')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bMcbTxUNfREI"
      },
      "outputs": [],
      "source": [
        "class QuestionGenerationDataset(Dataset):\n",
        "    def __init__(self, tokenizer, filepath, max_len_inp=512,max_len_out=512):\n",
        "        self.path = filepath\n",
        "\n",
        "        self.passage_column = \"context\"\n",
        "        self.answer = \"answer\"\n",
        "        self.question = \"question\"\n",
        "\n",
        "        # self.data = pd.read_csv(self.path)\n",
        "        self.data = pd.read_parquet(self.path).iloc[:2000,:]\n",
        "        self.max_len_input = max_len_inp\n",
        "        self.max_len_output = max_len_out\n",
        "        self.tokenizer = tokenizer\n",
        "        self.inputs = []\n",
        "        self.targets = []\n",
        "        self._build()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        source_ids = self.inputs[index][\"input_ids\"].squeeze()\n",
        "        target_ids = self.targets[index][\"input_ids\"].squeeze()\n",
        "\n",
        "        src_mask = self.inputs[index][\"attention_mask\"].squeeze()  #squeeze to get rid of the batch dimension\n",
        "        target_mask = self.targets[index][\"attention_mask\"].squeeze()  # convert [batch,dim] to [dim]\n",
        "\n",
        "\n",
        "        labels = copy.deepcopy(target_ids)\n",
        "        labels [labels==0] = -100\n",
        "\n",
        "        return {\"source_ids\": source_ids, \"source_mask\": src_mask, \"target_ids\": target_ids, \"target_mask\": target_mask,\"labels\":labels}\n",
        "\n",
        "    def _build(self):\n",
        "        for rownum,val in tqdm(self.data.iterrows()): # Iterating over the dataframe\n",
        "            passage,answer,target = val[self.passage_column],val[self.answer],val[self.question]\n",
        "\n",
        "            input_ = f\"context: {passage}\" # T5 Input format for question answering tasks\n",
        "            target = f\"question: {str(target)} answer: {answer}\" # Output format we require\n",
        "            # tokenize inputs\n",
        "            tokenized_inputs = self.tokenizer.batch_encode_plus(\n",
        "                [input_], max_length=self.max_len_input,padding='max_length',\n",
        "                truncation = True,return_tensors=\"pt\"\n",
        "            )\n",
        "            # tokenize targets\n",
        "            tokenized_targets = self.tokenizer.batch_encode_plus(\n",
        "                [target], max_length=self.max_len_output,padding='max_length',\n",
        "                truncation = True,\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "\n",
        "            self.inputs.append(tokenized_inputs)\n",
        "            self.targets.append(tokenized_targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ct6G1jKrlbwA"
      },
      "outputs": [],
      "source": [
        "train_path = '/content/drive/MyDrive/train_race.parquet' # change this accordingly\n",
        "validation_path = '/content/drive/MyDrive/validation_race.parquet'\n",
        "train_dataset = QuestionGenerationDataset(t5_tokenizer,train_path)\n",
        "validation_dataset = QuestionGenerationDataset(t5_tokenizer,validation_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exYklcOnvNak"
      },
      "outputs": [],
      "source": [
        "# Data Sample\n",
        "\n",
        "train_sample = train_dataset[10] # thanks to __getitem__\n",
        "decoded_train_input = t5_tokenizer.decode(train_sample['source_ids'])\n",
        "decoded_train_output = t5_tokenizer.decode(train_sample['target_ids'])\n",
        "\n",
        "print(decoded_train_input)\n",
        "print(decoded_train_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgpA1GU9wkgA"
      },
      "source": [
        "# Fine Tuning T5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10-Ap-LnwmeM"
      },
      "outputs": [],
      "source": [
        "import pytorch_lightning as pl\n",
        "from torch.optim import AdamW\n",
        "import argparse\n",
        "from transformers import (\n",
        "    get_linear_schedule_with_warmup\n",
        "  )\n",
        "\n",
        "class T5Tuner(pl.LightningModule):\n",
        "\n",
        "    def __init__(self,t5model, t5tokenizer,batchsize=4):\n",
        "        super().__init__()\n",
        "        self.model = t5model\n",
        "        self.tokenizer = t5tokenizer\n",
        "        self.batch_size = batchsize\n",
        "\n",
        "    def forward( self, input_ids, attention_mask=None,\n",
        "                decoder_attention_mask=None,\n",
        "                lm_labels=None):\n",
        "\n",
        "         outputs = self.model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            decoder_attention_mask=decoder_attention_mask,\n",
        "            labels=lm_labels,\n",
        "        )\n",
        "\n",
        "         return outputs\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        outputs = self.forward(\n",
        "            input_ids=batch[\"source_ids\"],\n",
        "            attention_mask=batch[\"source_mask\"],\n",
        "            decoder_attention_mask=batch['target_mask'],\n",
        "            lm_labels=batch['labels']\n",
        "        )\n",
        "\n",
        "        loss = outputs[0]\n",
        "        self.log('train_loss',loss)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        outputs = self.forward(\n",
        "            input_ids=batch[\"source_ids\"],\n",
        "            attention_mask=batch[\"source_mask\"],\n",
        "            decoder_attention_mask=batch['target_mask'],\n",
        "            lm_labels=batch['labels']\n",
        "        )\n",
        "\n",
        "        loss = outputs[0]\n",
        "        self.log(\"val_loss\",loss)\n",
        "        return loss\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(train_dataset, batch_size=self.batch_size,\n",
        "                          num_workers=2)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(validation_dataset,\n",
        "                          batch_size=self.batch_size,\n",
        "                          num_workers=2)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = AdamW(self.parameters(), lr=3e-4, eps=1e-8)\n",
        "        return optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0E5o7b1T3F7W"
      },
      "outputs": [],
      "source": [
        "model = T5Tuner(t5_model,t5_tokenizer)\n",
        "\n",
        "trainer = pl.Trainer(max_epochs = 3,accelerator=device)\n",
        "\n",
        "trainer.fit(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mbD88PJiFapv"
      },
      "outputs": [],
      "source": [
        "# saving the model\n",
        "!mkdir \"t5_tokenizer\"\n",
        "!mkdir \"t5_trained_model\"\n",
        "model.model.save_pretrained('t5_trained_model')\n",
        "t5_tokenizer.save_pretrained('t5_tokenizer')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVD6R7j2FWfU"
      },
      "source": [
        "# Inference / Predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run at this point"
      ],
      "metadata": {
        "id": "4a0ubAQC2MwH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxE0jOQCFYRq"
      },
      "outputs": [],
      "source": [
        "trained_model_path = '/content/drive/MyDrive/t5_trained_model'\n",
        "trained_tokenizer = '/content/drive/MyDrive/t5_tokenizer'\n",
        "device = 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bSAV5DP7GCzn"
      },
      "outputs": [],
      "source": [
        "model = T5ForConditionalGeneration.from_pretrained(trained_model_path)\n",
        "tokenizer = T5Tokenizer.from_pretrained(trained_tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYIpvIQaGSHO"
      },
      "source": [
        "Text Sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gO1uBvx8GGEf"
      },
      "outputs": [],
      "source": [
        "context =\"President Donald Trump said and predicted that some states would reopen this month.\"\n",
        "text = \"context: \"+context\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yd6K9qdY-Cr8"
      },
      "outputs": [],
      "source": [
        "context =\"Holi is considered as one of the most revered and celebrated festivals of India and it is celebrated in almost every part of the country. It is also sometimes called as the “festival of love” as on this day people get to unite together forgetting all resentments and all types of bad feeling towards each other. The great Indian festival lasts for a day and a night, which starts in the evening of Purnima or the Full Moon Day in the month of Falgun. It is celebrated with the name Holika Dahan or Choti Holi on first evening of the festival and the following day is called Holi. In different parts of the country it is known with different names. The vibrancy of colors is something that brings in a lot of positivity in our lives and Holi being the festival of colours is actually a day worth rejoicing. Holi is a famous Hindu festival that is celebrated in every part of India with utmost joy and enthusiasm. The ritual starts by lighting up the bonfire one day before the day of Holi and this process symbolizes the triumph of good over the bad. On the day of Holi people play with colours with their friends and families and in evening they show love and respect to their close ones with Abeer.\"\n",
        "\n",
        "text = \"context: \"+context\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XmEqkhNfGUVQ"
      },
      "outputs": [],
      "source": [
        "encoding = tokenizer.encode_plus(text,max_length =512,padding='max_length',\n",
        "                                 truncation = True,\n",
        "                                 return_tensors=\"pt\").to(device)\n",
        "print (encoding.keys())\n",
        "input_ids,attention_mask  = encoding[\"input_ids\"].to(device), encoding[\"attention_mask\"].to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mcXXoYDqGniS"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "\n",
        "beam_outputs = model.generate(\n",
        "    input_ids=input_ids,\n",
        "    attention_mask=attention_mask,\n",
        "    max_length=72, # How long the generated questions should be\n",
        "    early_stopping=True,\n",
        "    num_beams=5,\n",
        "    num_return_sequences=1\n",
        ")\n",
        "\n",
        "for beam_output in beam_outputs:\n",
        "    sent = tokenizer.decode(beam_output, skip_special_tokens=True,clean_up_tokenization_spaces=True)\n",
        "    print(sent)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yokqUsiu_nHl"
      },
      "source": [
        "# Deployment Demo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tq0tJKQb_o62"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet gradio==3.9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_67IW4GUCawH"
      },
      "outputs": [],
      "source": [
        "def get_question(sentence,mdl,tknizer):\n",
        "\n",
        "  ''' function to generate questions. Takes a sentence,answer,\n",
        "      model and tokenizer\n",
        "  '''\n",
        "\n",
        "  text = \"context: {}\".format(sentence)\n",
        "  #print (text)\n",
        "  max_len = 256\n",
        "  encoding = tknizer.encode_plus(text,max_length=max_len, pad_to_max_length=False,truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "  input_ids, attention_mask = encoding[\"input_ids\"], encoding[\"attention_mask\"]\n",
        "\n",
        "  outs = mdl.generate(input_ids=input_ids,\n",
        "                                  attention_mask=attention_mask,\n",
        "                                  early_stopping=True,\n",
        "                                  num_beams=5,\n",
        "                                  num_return_sequences=1,\n",
        "                                  no_repeat_ngram_size=2,\n",
        "                                  max_length=72)\n",
        "  #print(\"outputs\")\n",
        "  #print(outs)\n",
        "  for beam_output in outs:\n",
        "    sent = tokenizer.decode(beam_output, skip_special_tokens=True,clean_up_tokenization_spaces=True)\n",
        "    #print(sent)\n",
        "\n",
        "  dec = [tknizer.decode(ids,skip_special_tokens=True) for ids in outs]\n",
        "  #print(\"decs\")\n",
        "  #print(dec)\n",
        "\n",
        "  Question = dec[0]\n",
        "  #Question= Question.strip()\n",
        "  #Answer= Answer.strip()\n",
        "  #print(Answer)\n",
        "  index = Question.find(\"answer:\")\n",
        "\n",
        "  # Extract the question and answer based on the position\n",
        "  question = Question[10:index].strip()\n",
        "  answer = Question[index + len(\"answer:\"):].strip()\n",
        "  #print(\"Question1:\", question)\n",
        "  #print(\"Answer1:\", answer)\n",
        "\n",
        "  return  question,answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxQQ3wdnDJ4D",
        "outputId": "5a32dffa-956a-4fc7-b1b5-56539e19fa80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "context:  There had been a series of violent border skirmishes between the two countries after the 1959 Tibetan uprising, when India granted asylum to the Dalai Lama. Chinese military action grew increasingly aggressive after India rejected proposed Chinese diplomatic settlements throughout 1960–1962, with China resuming previously banned forward patrols in Ladakh after 30 April 1962. Amidst the Cuban Missile Crisis, China abandoned all attempts towards a peaceful resolution on 20 October 1962, invading disputed territory along the 3,225-kilometre (2,004 mi) border in Ladakh and across the McMahon Line in the northeastern frontier.[citation needed] Chinese troops pushed Indian forces back in both theatres, capturing all of their claimed territory in the western theatre and the Tawang Tract in the eastern theatre. The conflict ended when China unilaterally declared a ceasefire on 20 November 1962, and simultaneously announced its withdrawal to its pre-war position, the effective China–India border also known as the Line of Actual Control.\n",
            "question:  What happened after the 1959 Tibetan uprising?\n",
            "answer:  ['China granted asylum to the Dalai Lama.']\n"
          ]
        }
      ],
      "source": [
        "context = \"There had been a series of violent border skirmishes between the two countries after the 1959 Tibetan uprising, when India granted asylum to the Dalai Lama. Chinese military action grew increasingly aggressive after India rejected proposed Chinese diplomatic settlements throughout 1960–1962, with China resuming previously banned forward patrols in Ladakh after 30 April 1962. Amidst the Cuban Missile Crisis, China abandoned all attempts towards a peaceful resolution on 20 October 1962, invading disputed territory along the 3,225-kilometre (2,004 mi) border in Ladakh and across the McMahon Line in the northeastern frontier.[citation needed] Chinese troops pushed Indian forces back in both theatres, capturing all of their claimed territory in the western theatre and the Tawang Tract in the eastern theatre. The conflict ended when China unilaterally declared a ceasefire on 20 November 1962, and simultaneously announced its withdrawal to its pre-war position, the effective China–India border also known as the Line of Actual Control.\"\n",
        "\n",
        "print(\"context: \",context)\n",
        "ques,answer = get_question(context,model,tokenizer)\n",
        "print (\"question: \",ques)\n",
        "print (\"answer: \",answer)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = '''Economic agents can be individuals, businesses, organizations, or governments. Economic transactions occur when two groups or parties agree to the value or price of the transacted good or service, commonly expressed in a certain currency. However, monetary transactions only account for a small part of the economic domain.\n",
        "\n",
        "Economic activity is spurred by production which uses natural resources, labor and capital. It has changed over time due to technology, innovation (new products, services, processes, expanding markets, diversification of markets, niche markets, increases revenue functions) and changes in industrial relations (most notably child labor being replaced in some parts of the world with universal access to education).'''\n",
        "\n",
        "print(\"context: \",context)\n",
        "ques,answer = get_question(context,model,tokenizer)\n",
        "print (\"question: \",ques)\n",
        "print (\"answer: \",answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wtLG2dqbbqNG",
        "outputId": "87f6f57e-2c3d-4169-ef64-881e0e0299ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "context:  Economic agents can be individuals, businesses, organizations, or governments. Economic transactions occur when two groups or parties agree to the value or price of the transacted good or service, commonly expressed in a certain currency. However, monetary transactions only account for a small part of the economic domain.\n",
            "\n",
            "Economic activity is spurred by production which uses natural resources, labor and capital. It has changed over time due to technology, innovation (new products, services, processes, expanding markets, diversification of markets, niche markets, increases revenue functions) and changes in industrial relations (most notably child labor being replaced in some parts of the world with universal access to education).\n",
            "question:  Are there any other interesting aspects about this article?\n",
            "answer:  ['Economy agents can be individuals, businesses, organizations, or governments.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stripping paragraphs from paper"
      ],
      "metadata": {
        "id": "thRlRDzO5Tz0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2\n"
      ],
      "metadata": {
        "id": "vkeIRxcvfKf6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_input = \"\"\" The Sino-Indian War also known as the China–India War, Indo-China War, Indo-China War of 1962 or Sino-Indian War of 1962, was a military conflict between China and India that took place from October to November 1962. It was a military escalation of the Sino-Indian border dispute. Fighting occurred along India's border with China, in India's North-East Frontier Agency east of Bhutan, and in Aksai Chin west of Nepal.\n",
        "\n",
        "There had been a series of violent border skirmishes between the two countries after the 1959 Tibetan uprising, when India granted asylum to the Dalai Lama. Chinese military action grew increasingly aggressive after India rejected proposed Chinese diplomatic settlements throughout 1960–1962, with China resuming previously banned \"forward patrols\" in Ladakh after 30 April 1962.[9][10] Amidst the Cuban Missile Crisis, China abandoned all attempts towards a peaceful resolution on 20 October 1962, invading disputed territory along the 3,225-kilometre (2,004 mi) border in Ladakh and across the McMahon Line in the northeastern frontier.[citation needed] Chinese troops pushed Indian forces back in both theatres, capturing all of their claimed territory in the western theatre and the Tawang Tract in the eastern theatre. The conflict ended when China unilaterally declared a ceasefire on 20 November 1962, and simultaneously announced its withdrawal to its pre-war position, the effective China–India border (also known as the \"Line of Actual Control\").\n",
        "\n",
        "Much of the fighting comprised mountain warfare, entailing large-scale combat at altitudes of over 4,000 metres (13,000 feet).[citation needed] Notably, the war took place entirely on land, without the use of naval or air assets by either side.\n",
        "\n",
        "As the Sino-Soviet split deepened, the Soviet Union made a major effort to support India, especially with the sale of advanced MiG fighter-aircraft. Simultaneously, the United States and the United Kingdom refused to sell advanced weaponry to India, further compelling it to turn to the Soviets for military aid.[11][12] \"\"\""
      ],
      "metadata": {
        "id": "rkD-l1cOxYzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def strip_into_paragraphs(text):\n",
        "\n",
        "    # Split the text into individual lines\n",
        "    lines = text.split('\\n')\n",
        "\n",
        "    # Initialize an empty list to store paragraphs\n",
        "    paragraphs = []\n",
        "\n",
        "    # Initialize an empty string to store the current paragraph\n",
        "    current_paragraph = ''\n",
        "\n",
        "    # Iterate over each line\n",
        "    for line in lines:\n",
        "        # If the line is empty, consider it as a paragraph break\n",
        "        if not line.strip():\n",
        "            # Append the current paragraph to the list if it's not empty\n",
        "            if current_paragraph.strip():\n",
        "                paragraphs.append(current_paragraph.strip())\n",
        "            # Reset the current paragraph\n",
        "            current_paragraph = ''\n",
        "        else:\n",
        "            # Append the line to the current paragraph with a space\n",
        "            current_paragraph += line.strip() + ' '\n",
        "\n",
        "    # Append the last paragraph if it's not empty\n",
        "    if current_paragraph.strip():\n",
        "        paragraphs.append(current_paragraph.strip())\n",
        "\n",
        "    # Return the list of paragraphs\n",
        "    return paragraphs\n",
        "\n",
        "# Example usage\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "y3GTRwIIuj80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge_score\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "!pip install evaluate\n",
        "!pip install mauve-text"
      ],
      "metadata": {
        "id": "0YVwVbKi26Ak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bleu & Meteor & Rougue between Context and Questions"
      ],
      "metadata": {
        "id": "Gf0id3B4zo1e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = strip_into_paragraphs(text_input)\n",
        "!pip install rouge-score\n",
        "!pip install wordnet\n",
        "from nltk import word_tokenize\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from rouge_score import rouge_scorer\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "def U(K):\n",
        "    i=0\n",
        "    for paragraph in result :\n",
        "        scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL' ], use_stemmer=True)\n",
        "        i=i+1\n",
        "        print(\"context: \",paragraph)\n",
        "        ques,answer = get_question1(paragraph,model1,tokenizer1)\n",
        "        print (\"question: \",ques)\n",
        "        print (\"answer: \",answer)\n",
        "        BLEU = sentence_bleu([paragraph], ques, weights=(0.2, 0.2, 0.2, 0.2))\n",
        "        print(f\"\\nThe score for BLEU is: {BLEU}\")\n",
        "        scores = scorer.score(str(paragraph),\n",
        "                      str(ques))\n",
        "        print('rouge_score',scores)\n",
        "        m_score=0\n",
        "        for line in zip(paragraph, ques):\n",
        "                ref = word_tokenize(line[0])\n",
        "                #print(ref)\n",
        "                hypo = word_tokenize(line[1])\n",
        "                #print(hypo)\n",
        "                m_score += nltk.translate.meteor_score.meteor_score([ref], hypo)\n",
        "        print('Meteor Score:',m_score)\n",
        "        print('---')\n",
        "        if (i==K):\n",
        "          break"
      ],
      "metadata": {
        "id": "T9dH3Azyx9yS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bleu & Meteor & Rougue between Human Generated Question and Machine Generated Questions"
      ],
      "metadata": {
        "id": "PSKbNpox4N9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install  PyPDF2\n",
        "import PyPDF2\n",
        "!pip install pycryptodome\n",
        "reader = PyPDF2.PdfReader('/content/drive/MyDrive/001-HIDE-AND-SEEK-Free-Childrens-Book-By-Monkey-Pen.pdf')\n",
        "\n",
        "for pg in range(2,len(reader.pages)):\n",
        "      #print(\"New Para: \"+ str(pg))\n",
        "      context=reader.pages[pg].extract_text()\n",
        "      result = strip_into_paragraphs(context)\n",
        "!pip install wordnet\n",
        "from nltk import word_tokenize\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from rouge_score import rouge_scorer\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "#import PyPDF2\n",
        "#reader = PyPDF2.PdfReader('/content/drive/MyDrive/1411.1784.pdf')\n",
        "def U(K):\n",
        "    i=0\n",
        "    for paragraph in result :\n",
        "        scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL' ], use_stemmer=True)\n",
        "        i=i+1\n",
        "        print(\"context: \",paragraph)\n",
        "        ques,answer = get_question1(paragraph,model1,tokenizer1)\n",
        "        print (\"question: \",ques)\n",
        "        print (\"answer: \",answer)\n",
        "        reference = input(\"\\nPlease enter the corresponding human generated question from the context (you can use Chatgpt for this) and confirm with Enter: \")\n",
        "        BLEU = sentence_bleu([reference], ques, weights=(0.2, 0.2, 0.2, 0.2))\n",
        "        print(f\"\\nThe score for BLEU is: {BLEU}\")\n",
        "        scores = scorer.score(str(reference),\n",
        "                      str(ques))\n",
        "        print('rouge_score',scores)\n",
        "        m_score=0\n",
        "        for line in zip(reference, ques):\n",
        "                ref = word_tokenize(line[0])\n",
        "                #print(ref)\n",
        "                hypo = word_tokenize(line[1])\n",
        "                #print(hypo)\n",
        "                m_score += nltk.translate.meteor_score.meteor_score([ref], hypo)\n",
        "        print('Meteor Score:',m_score)\n",
        "        print('---')\n",
        "        if (i==K):\n",
        "          break"
      ],
      "metadata": {
        "id": "tzpjtMYk3b69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install PyPDF2\n",
        "import re\n",
        "import PyPDF2\n",
        "\n",
        "# Open the PDF file'/content/drive/MyDrive/1411.1784.pdf'\n",
        "pdf_file_path = '/content/drive/MyDrive/1411.1784.pdf'\n",
        "with open(pdf_file_path, \"rb\") as file:\n",
        "    pdf_reader = PyPDF2.PdfReader(file)\n",
        "    num_pages = len(pdf_reader.pages)\n",
        "\n",
        "    # Initialize an empty string to store the extracted text\n",
        "    extracted_text = \"\"\n",
        "\n",
        "    # Extract text from each page\n",
        "    for page_num in range(num_pages):\n",
        "        page = pdf_reader.pages[page_num]\n",
        "        extracted_text += page.extract_text()\n",
        "\n",
        "# Define regular expressions to match figures, tables, author names, and titles\n",
        "figure_regex = r\"\\b(Fig(?:ure)?\\.?\\s\\d+)\\b\"\n",
        "table_regex = r\"\\b(Table\\s\\d+)\\b\"\n",
        "author_regex = r\"\\bby\\s[\\w\\s]+\\b\"\n",
        "title_regex = r\"^\\s*[^\\n]+\\n\"\n",
        "\n",
        "# Remove figures, tables, author names, and titles\n",
        "\n",
        "extracted_text = re.sub(figure_regex, \"\", extracted_text, flags=re.IGNORECASE)\n",
        "\n",
        "extracted_text = re.sub(table_regex, \"\", extracted_text, flags=re.IGNORECASE)\n",
        "\n",
        "extracted_text = re.sub(author_regex, \"\", extracted_text, flags=re.IGNORECASE)\n",
        "\n",
        "#extracted_text = re.sub(title_regex, \"\", extracted_text, flags=re.MULTILINE)\n",
        "\n",
        "# Remove extra spaces and line breaks\n",
        "#extracted_text = re.sub(r\"\\s+\", \" \", extracted_text)\n",
        "\n",
        "# Print the resulting extracted text'''\n",
        "print(extracted_text)"
      ],
      "metadata": {
        "id": "Jxtb4R5857WI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install trafilatura\n",
        "import trafilatura\n",
        "downloaded = trafilatura.fetch_url('https://github.blog/2019-03-29-leader-spotlight-erin-spiceland/')\n",
        "trafilatura.extract(downloaded)\n",
        "# outputs main content and comments as plain text ...\n",
        "trafilatura.extract(downloaded, output_format=\"xml\", include_comments=False)"
      ],
      "metadata": {
        "id": "faKTXcAgvCnG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "U(1)"
      ],
      "metadata": {
        "id": "zYh2RuVUzxDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from PyPDF2 import PdfReader\n",
        "\n",
        "def extract_sections_from_pdf(file_path):\n",
        "    with open(file_path, 'rb') as file:\n",
        "        reader = PdfReader(file)\n",
        "        sections = {\n",
        "            'abstract': '',\n",
        "            'introduction': '',\n",
        "            'related_work': ''\n",
        "        }\n",
        "        current_section = None  # Variable to track the current section\n",
        "\n",
        "        for page_num in range(len(reader.pages)):\n",
        "            page = reader.pages[page_num]\n",
        "            text = page.extract_text()\n",
        "\n",
        "            # Search for section headings using regular expressions\n",
        "            if re.search(r\"(?i)\\babstract\\b\", text):\n",
        "                current_section = 'abstract'\n",
        "            if re.search(r\"(?i)\\bintroduction\\b\", text):\n",
        "                current_section = 'introduction'\n",
        "            #if re.search(r\"(?i)\\brelated work\\b\", text):\n",
        "                #current_section = 'related_work'\n",
        "\n",
        "            # If inside a desired section, append the text to the respective section variable\n",
        "            if current_section in sections:\n",
        "                sections[current_section] += text\n",
        "\n",
        "            # Check if we have reached the end of the desired sections\n",
        "            if current_section == 'related_work' and re.search(r\"(?i)\\bmethodology\\b|\\bexperimental results\\b\", text):\n",
        "                break\n",
        "\n",
        "        # Remove author information from sections\n",
        "        author_regex = r\"\\bby\\s[\\w\\s]+\\b\"\n",
        "        for section in sections:\n",
        "            sections[section] = re.sub(author_regex, \"\", sections[section], flags=re.IGNORECASE)\n",
        "\n",
        "        return sections\n",
        "\n",
        "# Example usage\n",
        "#pdf_file_path = '/content/drive/MyDrive/1411.1784.pdf'\n",
        "pdf_file_path = '/content/drive/MyDrive/s13369-023-07840-7 (1).pdf'\n",
        "extracted_sections = extract_sections_from_pdf(pdf_file_path)\n",
        "\n",
        "# Print the extracted sections\n",
        "for section, content in extracted_sections.items():\n",
        "    print(f\"--- {section.capitalize()} ---\")\n",
        "    print(content)\n",
        "    print('\\n')\n",
        "\n"
      ],
      "metadata": {
        "id": "uHZtIVzn2IJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Pre Processing"
      ],
      "metadata": {
        "id": "UpZ2IFqf7_N4"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5eLgL-_G8Fxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDaWwd7JNfjl"
      },
      "outputs": [],
      "source": [
        "#context = \"The Sino-Indian War also known as Indo-China War Indo-China War of 1962 or Sino-Indian War of 1962 took place between China and India from October to November 1962.\"\n",
        "def ques_answer(self,n):\n",
        "    import PyPDF2\n",
        "\n",
        "    # creating a pdf reader object\n",
        "    reader = PyPDF2.PdfReader('/content/drive/MyDrive/1411.1784.pdf')\n",
        "\n",
        "    # print the number of pages in pdf file\n",
        "    #print(len(reader.pages))\n",
        "\n",
        "    # print the text of the first page\n",
        "    #for pg in range(0,len(reader.pages)):\n",
        "    for pg in range(0,n):\n",
        "      #print(\"New Para: \"+ str(pg))\n",
        "      context=reader.pages[pg].extract_text()\n",
        "      #print(context)\n",
        "      ques, answer = get_question(context,model,tokenizer)\n",
        "      #print (\"question: \",ques)\n",
        "      #print (\"answer: \",answer)\n",
        "    return context, ques,answer\n",
        "    #return ques"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(ques_answer(1,5))"
      ],
      "metadata": {
        "id": "ySFzXIRr9wEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "context = gr.inputs.Textbox(lines=5,placeholder=\"Enter the Context\")\n",
        "# answer = gr.inputs.Textbox(lines=3, placeholder=\"Enter answer/keyword here...\")\n",
        "question = gr.outputs.Textbox( type=\"auto\", label=\"Question\")\n",
        "#answer = gr.outputs.Textbox( type=\"auto\", label=\"Answer\")\n",
        "\n",
        "def ques_answer(context):\n",
        "  return get_question(context,model,tokenizer)\n",
        "\n",
        "iface = gr.Interface(\n",
        "  fn=ques_answer,\n",
        "  inputs=[context],\n",
        "  outputs=question)\n",
        "\n",
        "\n",
        "iface.launch(debug=False,share=True)"
      ],
      "metadata": {
        "id": "kW-OhHQ0AMFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "-aZYlcXv8Lbt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install PyPDF2"
      ],
      "metadata": {
        "id": "sFL3a52Z1i8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YG8vauUyElvk"
      },
      "outputs": [],
      "source": [
        "import PyPDF2\n",
        "\n",
        "# creating a pdf reader object\n",
        "reader = PyPDF2.PdfReader('T5Paper_goodReference.pdf')\n",
        "\n",
        "# print the number of pages in pdf file\n",
        "print(len(reader.pages))\n",
        "\n",
        "# print the text of the first page\n",
        "for pg in range(0,len(reader.pages)):\n",
        "  print(\"New Para: \"+ str(pg))\n",
        "  print(reader.pages[pg].extract_text())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a=reader.pages[1].extract_text()\n",
        "print(a)"
      ],
      "metadata": {
        "id": "THSEpPzp15Yt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence_transformers"
      ],
      "metadata": {
        "id": "KjeSCFb0o2nj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This is a simple application for sentence embeddings: clustering\n",
        "\n",
        "Sentences are mapped to sentence embeddings and then k-mean clustering is applied.\n",
        "\"\"\"\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Corpus with example sentences\n",
        "#corpus = ['A man is eating food. A man is eating a piece of bread. A man is eating pasta.The girl is carrying a baby. The baby is carried by the womanA man is riding a horse. A man is riding a white horse on an enclosed ground. A monkey is playing drumsSomeone in a gorilla costume is playing a set of drums. A cheetah is running behind its prey. A cheetah chases prey on across a field.']\n",
        "#for pg in range(0,len(reader.pages)):\n",
        "for pg in range(0,5):\n",
        "  #print(\"New Para: \"+ str(pg))\n",
        "    context=reader.pages[pg].extract_text()\n",
        "    corpus1 = context.split('. ')\n",
        "    #print(corpus1)\n",
        "    corpus_embeddings = embedder.encode(corpus1)\n",
        "    num_clusters = 5\n",
        "    clustering_model = KMeans(n_clusters=num_clusters)\n",
        "    clustering_model.fit(corpus_embeddings)\n",
        "    cluster_assignment = clustering_model.labels_\n",
        "    #print(cluster_assignment)\n",
        "    clustered_sentences = [[] for i in range(num_clusters)]\n",
        "    for sentence_id, cluster_id in enumerate(cluster_assignment):\n",
        "      clustered_sentences[cluster_id].append(corpus1[sentence_id])\n",
        "      #print(clustered_sentences)\n",
        "\n",
        "    for i, cluster in enumerate(clustered_sentences):\n",
        "      print(\"Context \", i+1)\n",
        "      print(cluster)\n",
        "      print(\"\")\n",
        "      ques, answer = get_question(cluster,model,tokenizer)\n",
        "      print (\"question: \",ques)\n",
        "      print (\"answer: \",answer)\n"
      ],
      "metadata": {
        "id": "01Ugo9eg5re9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clustering & Question/Answer Pair Generation"
      ],
      "metadata": {
        "id": "aZSqjX6jG4KW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Get accuracy Percentage, what optimisation & strategies needs to be adopted to improve the system"
      ],
      "metadata": {
        "id": "i4wFopek46pf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ClusQuesGen(a,b,n):\n",
        "#a=1st page of the book, b=last page of the pdf, n=number of questions you want from each page\n",
        "      import PyPDF2\n",
        "      from sentence_transformers import SentenceTransformer\n",
        "      from sklearn.cluster import KMeans\n",
        "\n",
        "      embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "      reader = PyPDF2.PdfReader('/content/drive/MyDrive/2006.09595 (1).pdf')\n",
        "\n",
        "      # Corpus with example sentences\n",
        "      #corpus = ['A man is eating food. A man is eating a piece of bread. A man is eating pasta.The girl is carrying a baby. The baby is carried by the womanA man is riding a horse. A man is riding a white horse on an enclosed ground. A monkey is playing drumsSomeone in a gorilla costume is playing a set of drums. A cheetah is running behind its prey. A cheetah chases prey on across a field.']\n",
        "      #for pg in range(0,len(reader.pages)):\n",
        "      ques_answer=[]\n",
        "      for pg in range(a,b):\n",
        "        #print(\"New Para: \"+ str(pg))\n",
        "          context=reader.pages[pg].extract_text()\n",
        "          corpus1 = context.split('. ')\n",
        "          #print(corpus1)\n",
        "          corpus_embeddings = embedder.encode(corpus1)\n",
        "          num_clusters = n\n",
        "          clustering_model = KMeans(n_clusters=num_clusters)\n",
        "          clustering_model.fit(corpus_embeddings)\n",
        "          cluster_assignment = clustering_model.labels_\n",
        "          #print(cluster_assignment)\n",
        "          clustered_sentences = [[] for i in range(num_clusters)]\n",
        "          for sentence_id, cluster_id in enumerate(cluster_assignment):\n",
        "            clustered_sentences[cluster_id].append(corpus1[sentence_id])\n",
        "            #print(clustered_sentences)\n",
        "          for i, cluster in enumerate(clustered_sentences):\n",
        "            print(\"Context \", i+1)\n",
        "            print(cluster)\n",
        "            print(\"\")\n",
        "            ques, answer = get_question(cluster,model,tokenizer)\n",
        "            ques_answer.append([cluster, i+1, ques, answer])\n",
        "            #return cluster, ques, answer\n",
        "            #print(ques_answer)\n",
        "            print (\"question: \",ques)\n",
        "            print (\"answer: \",answer)\n",
        "      #return cluster, ques, answer\n"
      ],
      "metadata": {
        "id": "mH2SKKySWSjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ClusQuesGen(3,4,2)\n"
      ],
      "metadata": {
        "id": "se4TmCzVgEti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "#context = gr.inputs.Textbox(lines=5,placeholder=\"Enter paragraph/context here...\")\n",
        "# answer = gr.inputs.Textbox(lines=3, placeholder=\"Enter answer/keyword here...\")\n",
        "a = gr.inputs.Textbox(lines=5,placeholder=\"Enter starting page number of the text:\")\n",
        "b = gr.inputs.Textbox(lines=5,placeholder=\"Enter ending page number of the text:\")\n",
        "n = gr.inputs.Textbox(lines=5,placeholder=\"Enter number of questions needs to be generated from each page:\")\n",
        "#context = gr.outputs.Textbox( type=\"auto\", label=\"cluster\")\n",
        "#question = gr.outputs.Textbox( type=\"auto\", label=\"Question\")\n",
        "#answer = gr.outputs.Textbox( type=\"auto\", label=\"Answer\")\n",
        "\n",
        "#def generate_question(a,b,n):\n",
        "def generate_question(context):\n",
        "  return get_question(cluster,model,tokenizer)\n",
        "  #return ClusQuesGen(a,b,n)\n",
        "\n",
        "iface = gr.Interface(\n",
        "  fn=generate_question,\n",
        "  inputs=[a,b,n],\n",
        "  outputs=[\"text\", \"text\", \"text\"])\n",
        "  outputs=[\"text\"])\n",
        "iface.launch(debug=False,share=True)"
      ],
      "metadata": {
        "id": "tEjdVhf3WFdu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "context = gr.inputs.Textbox(lines=5,placeholder=\"Enter paragraph/context here...\")\n",
        "# answer = gr.inputs.Textbox(lines=3, placeholder=\"Enter answer/keyword here...\")\n",
        "question = gr.outputs.Textbox( type=\"auto\", label=\"Question\")\n",
        "#answer = gr.outputs.Textbox( type=\"auto\", label=\"Answer\")\n",
        "\n",
        "def generate_question(context):\n",
        "  return get_question(context,model,tokenizer)\n",
        "\n",
        "iface = gr.Interface(\n",
        "  fn=generate_question,\n",
        "  inputs=[context],\n",
        "  outputs=question)\n",
        "  #outputs=answer)\n",
        "\n",
        "iface.launch(debug=False,share=True)"
      ],
      "metadata": {
        "id": "6Cw_VPVoi-uv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "def greet(name, is_morning, temperature):\n",
        "    salutation = \"Good morning\" if is_morning else \"Good evening\"\n",
        "    greeting = f\"{salutation} {name}. It is {temperature} degrees today\"\n",
        "    celsius = (temperature - 32) * 5 / 9\n",
        "    return greeting, round(celsius, 2)\n",
        "\n",
        "demo = gr.Interface(\n",
        "    fn=greet,\n",
        "    inputs=[\"text\", \"checkbox\", gr.Slider(0, 100)],\n",
        "    outputs=[\"text\", \"number\"],\n",
        ")\n",
        "demo.launch()"
      ],
      "metadata": {
        "id": "jJOZ56BxWRHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scores on different Metrics"
      ],
      "metadata": {
        "id": "TQUd_-rRlyi7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge_score\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "!pip install evaluate\n",
        "!pip install mauve-text"
      ],
      "metadata": {
        "id": "NrDZ8Fefk_-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "#Bleu Score\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "reference = [\n",
        "    'this is a dog'.split(),\n",
        "    'it is dog'.split(),\n",
        "    'dog it is'.split(),\n",
        "    'a dog, it is'.split()\n",
        "]\n",
        "#reference\n",
        "candidate = 'it is dog'.split()\n",
        "print('BLEU score -> {}'.format(sentence_bleu(reference, candidate )))\n",
        "\n",
        "candidate = 'it is a dog'.split()\n",
        "print('BLEU score -> {}'.format(sentence_bleu(reference, candidate)))\n",
        "\n",
        "#rouge_scorer\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
        "'''scores = scorer.score('The quick brown fox jumps over the lazy dog',\n",
        "                      'The quick brown dog jumps on the log.')'''\n",
        "scores = scorer.score('On which factors does the economy functions?',\n",
        "                      'What is the social domain of an economy')\n",
        "print('rouge_score',scores)\n",
        "\n",
        "\n",
        "#meteor score\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "refs=[\"On which factors does the economy functions?\"]\n",
        "hypos=[\"What is the social domain of an economy\"]\n",
        "m_score=0\n",
        "for line in zip(refs, hypos):\n",
        "        ref = word_tokenize(line[0])\n",
        "        hypo = word_tokenize(line[1])\n",
        "        m_score += nltk.translate.meteor_score.meteor_score([ref], hypo)\n",
        "print('Meteor Score:',m_score)\n",
        "\n",
        "\n",
        "#Mauve Score\n",
        "from evaluate import load\n",
        "mauve = load('mauve')\n",
        "#predictions = [\"hello world\", \"goodnight moon\"]\n",
        "#references = [\"hello there\", \"general kenobi\"]\n",
        "predictions = [\"On which factors does the economy functions?\"]\n",
        "references = [\"What is the social domain of an economy\"]\n",
        "mauve_results = mauve.compute(predictions=predictions, references=references)\n",
        "print('Mauve:',mauve_results.mauve)"
      ],
      "metadata": {
        "id": "NC6fY3GJkyb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Import the word_tokenize and sentence_bleu methods from NLTK.\n",
        "from nltk import word_tokenize\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "#Enter the machine-translated sentence (hypothesis) and the human reference translation (reference).\n",
        "hypothesis = input(\"\\nPlease enter a machine-translated sentence and confirm with Enter: \")\n",
        "reference = input(\"\\nPlease enter the corresponding human reference translation and confirm with Enter: \")\n",
        "\n",
        "#Tokenize the two sentences and store the individual tokes in two lists (one for the hypothesis tokens and one for the reference tokens).\n",
        "hypothesis = word_tokenize(hypothesis)\n",
        "reference = word_tokenize(reference)\n",
        "\n",
        "#Calculate and print BLEU-1, using 1-grams as highest-order n-grams\n",
        "#Reference is placed in [square brackets] because you can score the machine-translated sentence against multiple references (applies to all BLEU score calculations listed here).\n",
        "#The weights are set so that calculation is based solely on 1-gram precision.\n",
        "BLEU_1 = sentence_bleu([reference], hypothesis, weights=(1, 0, 0, 0))\n",
        "print(f\"\\nThe score for BLEU-1 is: {BLEU_1}\")\n",
        "\n",
        "#Calculate and print BLEU-2, using 2-grams as highest-order n-grams.\n",
        "#The weights are set so that calculation is based on 1-gram and 2-gram precisions.\n",
        "BLEU_2 = sentence_bleu([reference], hypothesis, weights=(0.5, 0.5, 0, 0))\n",
        "print(f\"\\nThe score for BLEU-2 is: {BLEU_2}\")\n",
        "\n",
        "#Calculate and print BLEU-3, using 3-grams as highest-order n-grams.\n",
        "#The weights are set so that calculation is based on 1-gram, 2-gram and 3-gram precisions.\n",
        "BLEU_3 = sentence_bleu([reference], hypothesis, weights=(0.33, 0.33, 0.33, 0))\n",
        "print(f\"\\nThe score for BLEU-3 is: {BLEU_3}\")\n",
        "\n",
        "#Calculate and print BLEU-4, using 4-grams as highest-order n-grams.\n",
        "#The weights are set so that calculation is based on 1-gram, 2-gram, 3-gram and 4-gram precisions.\n",
        "BLEU_4 = sentence_bleu([reference], hypothesis, weights=(0.25, 0.25, 0.25, 0.25))\n",
        "print(f\"\\nThe score for BLEU-4 is: {BLEU_4}\")\n",
        "\n",
        "#Calculate and print BLEU-5, using 5-grams as highest-order n-grams.\n",
        "#The weights are set so that calculation is based on 1-gram, 2-gram, 3-gram, 4-gram and 5-gram precisions.\n",
        "BLEU_5 = sentence_bleu([reference], hypothesis, weights=(0.2, 0.2, 0.2, 0.2, 0.2))\n",
        "print(f\"\\nThe score for BLEU-5 is: {BLEU_5}\")"
      ],
      "metadata": {
        "id": "b1ZYQqHnWwAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word 2 Vec"
      ],
      "metadata": {
        "id": "R8sZKVJmeTv4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install word2vec\n",
        "!pip install gensim"
      ],
      "metadata": {
        "id": "SCyDh6xxmrcB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences='this is a dog'"
      ],
      "metadata": {
        "id": "Oz-6RVQCnaZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "from gensim.utils import simple_preprocess\n",
        "sentence = \"this is an example sentence.\"\n",
        "tokenized_sentence = simple_preprocess(sentence)\n",
        "#print(tokenized_sentence)\n",
        "model = Word2Vec(sentences, min_count=1)\n",
        "vector = model.wv[\"This\"]\n",
        "similarity_score = model.wv.similarity(\"This#\", \"sentence\")\n",
        "print(similarity_score)"
      ],
      "metadata": {
        "id": "9oAvK-h2mw9G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataset(filename,  verbose = False):\n",
        "  data  = pd.read_csv(filename, skipinitialspace=True, usecols=['Context','Question','Answer'] )\n",
        "  result_df1  = pd.DataFrame(columns = ['context', 'question','answer'])\n",
        "  result_df1 = data[0:177, : ]\n",
        "  return result_df1"
      ],
      "metadata": {
        "id": "sXzXMQoqudoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df3"
      ],
      "metadata": {
        "id": "BhOG_lrlau07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename = r\"Dataset - Sheet1.csv\"\n",
        "\n",
        "new_df = create_dataset(filename)\n",
        "#new_df = create_dataset(df3)\n",
        "print(new_df)"
      ],
      "metadata": {
        "id": "eh8bkYfPs24H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df3"
      ],
      "metadata": {
        "id": "hSRKxkCybtLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class QuestionGenerationDataset_New(Dataset):\n",
        "    def __init__(self, tokenizer, data, max_len_inp=512,max_len_out=512):\n",
        "        self.passage_column = \"Context\"\n",
        "        self.answer = \"Answer\"\n",
        "        self.question = \"Question\"\n",
        "\n",
        "        # self.data = pd.read_csv(self.path)\n",
        "        self.data = data\n",
        "        self.max_len_input = max_len_inp\n",
        "        self.max_len_output = max_len_out\n",
        "        self.tokenizer = tokenizer\n",
        "        self.inputs = []\n",
        "        self.targets = []\n",
        "        self._build()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        source_ids = self.inputs[index][\"input_ids\"].squeeze()\n",
        "        target_ids = self.targets[index][\"input_ids\"].squeeze()\n",
        "\n",
        "        src_mask = self.inputs[index][\"attention_mask\"].squeeze()  #squeeze to get rid of the batch dimension\n",
        "        target_mask = self.targets[index][\"attention_mask\"].squeeze()  # convert [batch,dim] to [dim]\n",
        "\n",
        "\n",
        "        labels = copy.deepcopy(target_ids)\n",
        "        labels [labels==0] = -100\n",
        "\n",
        "        return {\"source_ids\": source_ids, \"source_mask\": src_mask, \"target_ids\": target_ids, \"target_mask\": target_mask,\"labels\":labels}\n",
        "\n",
        "    def _build(self):\n",
        "        for rownum,val in tqdm(self.data.iterrows()): # Iterating over the dataframe\n",
        "\n",
        "            passage,answer,target = val[self.passage_column],val[self.answer],val[self.question]\n",
        "\n",
        "            input_ = f\"context: {passage}\" # T5 Input format for question answering tasks\n",
        "            target = f\"question: {str(target)} answer: {answer}\" # Output format we require\n",
        "            # tokenize inputs\n",
        "            tokenized_inputs = self.tokenizer.batch_encode_plus(\n",
        "                [input_], max_length=self.max_len_input,padding='max_length',\n",
        "                truncation = True,return_tensors=\"pt\"\n",
        "            )\n",
        "            # tokenize targets\n",
        "            tokenized_targets = self.tokenizer.batch_encode_plus(\n",
        "                [target], max_length=self.max_len_output,padding='max_length',\n",
        "                truncation = True,\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "\n",
        "            self.inputs.append(tokenized_inputs)\n",
        "            self.targets.append(tokenized_targets)"
      ],
      "metadata": {
        "id": "7qpbCIwZu05F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset1 = QuestionGenerationDataset_New(t5_tokenizer,new_df.sample(frac = 0.7))\n",
        "validation_dataset1 = QuestionGenerationDataset_New(t5_tokenizer,new_df.drop(new_df.sample(frac = 0.7).index))"
      ],
      "metadata": {
        "id": "2cA_6Ph4vGmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset1 = QuestionGenerationDataset_New(t5_tokenizer,df3.sample(frac = 0.7))\n",
        "validation_dataset1 = QuestionGenerationDataset_New(t5_tokenizer,df3.drop(df3.sample(frac = 0.7).index))"
      ],
      "metadata": {
        "id": "r-7j7KCmc5KH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pytorch_lightning as pl\n",
        "from torch.optim import AdamW\n",
        "import argparse\n",
        "from transformers import (\n",
        "    get_linear_schedule_with_warmup\n",
        "  )\n",
        "\n",
        "class T5Tuner1(pl.LightningModule):\n",
        "\n",
        "    def __init__(self,t5model, t5tokenizer,batchsize=4):\n",
        "        super().__init__()\n",
        "        self.model = t5model\n",
        "        self.tokenizer = t5tokenizer\n",
        "        self.batch_size = batchsize\n",
        "\n",
        "    def forward( self, input_ids, attention_mask=None,\n",
        "                decoder_attention_mask=None,\n",
        "                lm_labels=None):\n",
        "\n",
        "         outputs = self.model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            decoder_attention_mask=decoder_attention_mask,\n",
        "            labels=lm_labels,\n",
        "        )\n",
        "\n",
        "         return outputs\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        outputs = self.forward(\n",
        "            input_ids=batch[\"source_ids\"],\n",
        "            attention_mask=batch[\"source_mask\"],\n",
        "            decoder_attention_mask=batch['target_mask'],\n",
        "            lm_labels=batch['labels']\n",
        "        )\n",
        "\n",
        "        loss = outputs[0]\n",
        "        self.log('train_loss',loss)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        outputs = self.forward(\n",
        "            input_ids=batch[\"source_ids\"],\n",
        "            attention_mask=batch[\"source_mask\"],\n",
        "            decoder_attention_mask=batch['target_mask'],\n",
        "            lm_labels=batch['labels']\n",
        "        )\n",
        "\n",
        "        loss = outputs[0]\n",
        "        self.log(\"val_loss\",loss)\n",
        "        return loss\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(train_dataset1, batch_size=self.batch_size,\n",
        "                          num_workers=2)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(validation_dataset1,\n",
        "                          batch_size=self.batch_size,\n",
        "                          num_workers=2)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = AdamW(self.parameters(), lr=3e-4, eps=1e-8)\n",
        "        return optimizer"
      ],
      "metadata": {
        "id": "Ix8IV4JRvSqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trained_model_path = '/content/drive/MyDrive/t5_trained_model'\n",
        "trained_tokenizer = '/content/drive/MyDrive/t5_tokenizer'\n",
        "device = 'cpu'"
      ],
      "metadata": {
        "id": "mxUzQmTdwcJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = T5ForConditionalGeneration.from_pretrained(trained_model_path)\n",
        "tokenizer = T5Tokenizer.from_pretrained(trained_tokenizer)\n",
        "\n",
        "model1 = T5Tuner1(model,tokenizer)\n",
        "\n",
        "trainer = pl.Trainer(max_epochs = 10,accelerator=device, log_every_n_steps=10)\n",
        "\n",
        "trainer.fit(model1)"
      ],
      "metadata": {
        "id": "OdutTq2XvbqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# saving the model\n",
        "#!mkdir \"t5_tokenizer1\"\n",
        "#!mkdir \"t5_trained_model1\"\n",
        "model1.model.save_pretrained('t5_trained_model1')\n",
        "t5_tokenizer.save_pretrained('t5_tokenizer1')"
      ],
      "metadata": {
        "id": "xtp99qcW1MmV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10 Epochs"
      ],
      "metadata": {
        "id": "DyqkVp0l1vdn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model1.model.save_pretrained('/content/drive/MyDrive/t5_trained_model2')\n",
        "t5_tokenizer.save_pretrained('/content/drive/MyDrive/t5_tokenizer2')"
      ],
      "metadata": {
        "id": "FkqClY8gh1tn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model1.model.save_pretrained('/content/drive/MyDrive/t5_trained_model1')\n",
        "t5_tokenizer.save_pretrained('/content/drive/MyDrive/t5_tokenizer1')"
      ],
      "metadata": {
        "id": "U5qg5nzo2YS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inference"
      ],
      "metadata": {
        "id": "UeUdfbtp4-N5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trained_model_path1 = '/content/drive/MyDrive/t5_trained_model2'\n",
        "trained_tokenizer1 = '/content/drive/MyDrive/t5_tokenizer2'\n",
        "device = 'cpu'"
      ],
      "metadata": {
        "id": "SDd7XYO55Ac6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model1 = T5ForConditionalGeneration.from_pretrained(trained_model_path1)\n",
        "tokenizer1 = T5Tokenizer.from_pretrained(trained_tokenizer1)"
      ],
      "metadata": {
        "id": "qm_cjfgV5KCB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_question1(sentence,mdl,tknizer):\n",
        "\n",
        "  ''' function to generate questions. Takes a sentence,answer,\n",
        "      model and tokenizer\n",
        "  '''\n",
        "\n",
        "  text = \"context: {}\".format(sentence)\n",
        "  #print (text)\n",
        "  max_len = 256\n",
        "  encoding = tknizer.encode_plus(text,max_length=max_len, pad_to_max_length=False,truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "  input_ids, attention_mask = encoding[\"input_ids\"], encoding[\"attention_mask\"]\n",
        "\n",
        "  outs = mdl.generate(input_ids=input_ids,\n",
        "                                  attention_mask=attention_mask,\n",
        "                                  early_stopping=True,\n",
        "                                  num_beams=5,\n",
        "                                  num_return_sequences=1,\n",
        "                                  no_repeat_ngram_size=2,\n",
        "                                  max_length=72)\n",
        "  #print(\"outputs\")\n",
        "  #print(outs)\n",
        "  for beam_output in outs:\n",
        "    sent = tokenizer.decode(beam_output, skip_special_tokens=True,clean_up_tokenization_spaces=True)\n",
        "    #print(sent)\n",
        "\n",
        "  dec = [tknizer.decode(ids,skip_special_tokens=True) for ids in outs]\n",
        "  #print(\"decs\")\n",
        "  #print(dec)\n",
        "\n",
        "  Question = dec[0]\n",
        "  #Question= Question.strip()\n",
        "  #Answer= Answer.strip()\n",
        "  #print(Answer)\n",
        "  index = Question.find(\"answer:\")\n",
        "\n",
        "  # Extract the question and answer based on the position\n",
        "  question = Question[10:index].strip()\n",
        "  answer = Question[index + len(\"answer:\"):].strip()\n",
        "  #print(\"Question1:\", question)\n",
        "  #print(\"Answer1:\", answer)\n",
        "\n",
        "  return  question,answer"
      ],
      "metadata": {
        "id": "aiN_On2y3r-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context = \"The Sino-Indian War also known as the China–India War, Indo-China War, Indo-China War of 1962 or Sino-Indian War of 1962, was a military conflict between China and India that took place from October to November 1962. It was a military escalation of the Sino-Indian border dispute. Fighting occurred along India's border with China, in India's North-East Frontier Agency east of Bhutan, and in Aksai Chin west of Nepal.\"\n",
        "\n",
        "print(\"context: \",context)\n",
        "ques,answer = get_question1(context,model1,tokenizer1)\n",
        "print (\"question: \",ques)\n",
        "print (\"answer: \",answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3QcY4Sti30s0",
        "outputId": "cb32943a-32f4-4e65-9fad-f78874310ac0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "context:  The Sino-Indian War also known as the China–India War, Indo-China War, Indo-China War of 1962 or Sino-Indian War of 1962, was a military conflict between China and India that took place from October to November 1962. It was a military escalation of the Sino-Indian border dispute. Fighting occurred along India's border with China, in India's North-East Frontier Agency east of Bhutan, and in Aksai Chin west of Nepal.\n",
            "question:  What was the military escalation of the Sino-Indian border dispute between China and India?\n",
            "answer:  The conflict occurred along India's border with China, in India’s North-East Frontier Agency east of Bhutan, and in Aksai Chin west of Nepal.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = '''Economic agents can be individuals, businesses, organizations, or governments. Economic transactions occur when two groups or parties agree to the value or price of the transacted good or service, commonly expressed in a certain currency. However, monetary transactions only account for a small part of the economic domain.\n",
        "\n",
        "Economic activity is spurred by production which uses natural resources, labor and capital. It has changed over time due to technology, innovation new products, services, processes, expanding markets, diversification of markets, niche markets, increases revenue functions and changes in industrial relations most notably child labor being replaced in some parts of the world with universal access to education.'''\n",
        "\n",
        "print(\"context: \",context)\n",
        "ques,answer = get_question1(context,model1,tokenizer1)\n",
        "print (\"question: \",ques)\n",
        "print (\"answer: \",answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cONSB4RgcfNT",
        "outputId": "728f8898-4fc6-4701-d66f-0829cfa0246b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "context:  Economic agents can be individuals, businesses, organizations, or governments. Economic transactions occur when two groups or parties agree to the value or price of the transacted good or service, commonly expressed in a certain currency. However, monetary transactions only account for a small part of the economic domain.\n",
            "\n",
            "Economic activity is spurred by production which uses natural resources, labor and capital. It has changed over time due to technology, innovation new products, services, processes, expanding markets, diversification of markets, niche markets, increases revenue functions and changes in industrial relations most notably child labor being replaced in some parts of the world with universal access to education.\n",
            "question:  How does the economic domain change?\n",
            "answer:  Economic activity is spurred by production which uses natural resources, labor and capital. It has changed over time due to technology, innovation new products, services, processes, expanding markets, diversification of markets and niche markets. This increases revenue functions and changes in industrial relations including child labor being replaced in some\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context='''1.2 DESIGNING A LEARNING SYSTEM\n",
        "In order to illustrate some of the basic design issues and approaches to machine\n",
        "learning, let us consider designing a program to learn to play checkers, with\n",
        "the goal of entering it in the world checkers tournament. We adopt the obvious\n",
        "performance measure: the percent of games it wins in this world tournament.\n",
        "1.2.1 Choosing the Training Experience\n",
        "The first design choice we face is to choose the type of training experience from\n",
        "which our system will learn. The type of training experience available can have a\n",
        "significant impact on success or failure of the learner. One key attribute is whether\n",
        "the training experience provides direct or indirect feedback regarding the choices\n",
        "made by the performance system. For example, in learning to play checkers, the\n",
        "system might learn from direct training examples consisting of individual checkers\n",
        "board states and the correct move for each. Alternatively, it might have available\n",
        "only indirect information consisting of the move sequences and final outcomes\n",
        "of various games played. In this later case, information about the correctness\n",
        "of specific moves early in the game must be inferred indirectly from the fact\n",
        "that the game was eventually won or lost. Here the learner faces an additional\n",
        "problem of credit assignment, or determining the degree to which each move in\n",
        "the sequence deserves credit or blame for the final outcome. Credit assignment can\n",
        "be a particularly difficult problem because the game can be lost even when early\n",
        "moves are optimal, if these are followed later by poor moves. Hence, learning from\n",
        "direct training feedback is typically easier than learning from indirect feedback.\n",
        "A second important attribute of the training experience is the degree to which\n",
        "the learner controls the sequence of training examples. For example, the learner\n",
        "might rely on the teacher to select informative board states and to provide the\n",
        "correct move for each. Alternatively, the learner might itself propose board states\n",
        "that it finds particularly confusing and ask the teacher for the correct move. Or the\n",
        "learner may have complete control over both the board states and (indirect) training\n",
        "classifications, as it does when it learns by playing against itself with no teacher\n",
        "present. Notice in this last case the learner may choose between experimenting\n",
        "with novel board states that it has not yet considered, or honing its skill by playing\n",
        "minor variations of lines of play it currently finds most promising. Subsequent\n",
        "chapters consider a number of settings for learning, including settings in which\n",
        "training experience is provided by a random process outside the learner's control,\n",
        "settings in which the learner may pose various types of queries to an expert teacher,\n",
        "and settings in which the learner collects training examples by autonomously\n",
        "exploring its environment.\n",
        "A third important attribute of the training experience is how well it represents the distribution of examples over which the final system performance P must\n",
        "be measured. In general, learning is most reliable when the training examples follow a distribution similar to that of future test examples. In our checkers learning\n",
        "scenario, the performance metric P is the percent of games the system wins in\n",
        "the world tournament. If its training experience E consists only of games played\n",
        "against itself, there is an obvious danger that this training experience might not\n",
        "be fully representative of the distribution of situations over which it will later be\n",
        "tested. For example, the learner might never encounter certain crucial board states\n",
        "that are very likely to be played by the human checkers champion. In practice,\n",
        "it is often necessary to learn from a distribution of examples that is somewhat\n",
        "different from those on which the final system will be evaluated (e.g., the world\n",
        "checkers champion might not be interested in teaching the program!). Such situations are problematic because mastery of one distribution of examples will not\n",
        "necessary lead to strong performance over some other distribution. We shall see\n",
        "that most current theory of machine learning rests on the crucial assumption that\n",
        "the distribution of training examples is identical to the distribution of test examples. Despite our need to make this assumption in order to obtain theoretical\n",
        "results, it is important to keep in mind that this assumption must often be violated\n",
        "in practice.\n",
        "To proceed with our design, let us decide that our system will train by\n",
        "playing games against itself. This has the advantage that no external trainer need\n",
        "be present, and it therefore allows the system to generate as much training data\n",
        "as time permits. We now have a fully specified learning task. '''\n",
        "ques,answer = get_question1(context,model1,tokenizer1)\n",
        "print (\"question: \",ques)\n",
        "print (\"answer: \",answer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "226qdGplpc1D",
        "outputId": "511d7112-ca3c-471b-cf7b-84c9e37a6858"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "question:  What are some basic design issues and approaches to machine learning?\n",
            "answer:  Choosing the Training Experience The first design choice we face is to choose the type of training experience from which the system will learn. The design experience provides direct or indirect feedback regarding the choices made by the performance system. Alternatively, it may have available only indirect information consisting\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#context = \"According to one embodiment of the present dis closure , a question - answering learning method is prtences among N sentences , wherein the classifier module includes several classifiers each representing different question - answering models , and both of the N and the N1 are positive integers . At least one corresponding sentence type of each of the N2 unlabeled sentences among the N sentences is determined by each of the classifiers , wherein the N2 is a positive integer . In a consistency evaluation process , N3 sentences are selected from the N2 unlabeled sentences by a consistency evaluation module according to a degree of consistency of determined results of the classifiers , wherein the determined results of the N3 sentences are determined by the classifiers and are inconsistent , and the N3 is a positive integer . In a complementarity evaluation process , N4 mutually complementary sentences are selected as to - be - labeled sentences from the N3 sentences by a complementarity evaluation module , wherein the N4 is a positive integer . After the N4 selected to - be - labeled sentences are labeled , the classifiers of the classifier module are re - created by the classifier generation module according to the N1 labeled sentences and the N4 selected to - be - labeled sentences . At least one of previously created classifiers is added to the classifier module to be members of the classifier module by a classifier evaluation module. According to another embodiment of the present disclosure , a question - answering learning system is provided . The question - answering learning system includes a classifier generation module , a consistency evaluation module , a complementarity evaluation module and a classifier evaluation module . The classifier generation module is configured to : create a classifier module according to N1 labeled sentences among N sentences , wherein the classifier module includes several classifiers each representing different ques tion - answering models , and both of the N and the N1 are positive integers . Each of the classifiers determines at least one corresponding sentence type of each of the N2 unlabeled sentences among the N sentences , wherein the N2 is a positive integer . The consistency evaluation module is configured to : select , in a consistency evaluation process , N3 sentences from the N2 unlabeled sentences according to a degree of consistency of determined results of the classifiers , wherein the determined results of the N3 sentences are determined by the classifiers and are inconsistent , and the N3 is a positive integer . \"\n",
        "#context = \"In a complementarity evaluation process , N4 mutually complementary sentences are selected as to - be - labeled sentences from the N3 sentences by a complementarity evaluation module , wherein the N4 is a positive integer . After the N4 selected to - be - labeled sentences are labeled , the classifiers of the classifier module are re - created by the classifier generation module according to the N1 labeled sentences and the N4 selected to - be - labeled sentences . At least one of previously created classifiers is added to the classifier module to be members of the classifier module by a classifier evaluation module. According to another embodiment of the present disclosure , a question - answering learning system is provided . The question - answering learning system includes a classifier generation module , a consistency evaluation module , a complementarity evaluation module and a classifier evaluation module . The classifier generation module is configured to : create a classifier module according to N1 labeled sentences among N sentences , wherein the classifier module includes several classifiers each representing different ques tion - answering models , and both of the N and the N1 are positive integers . Each of the classifiers determines at least one corresponding sentence type of each of the N2 unlabeled sentences among the N sentences , wherein the N2 is a positive integer . The consistency evaluation module is configured to : select , in a consistency evaluation process , N3 sentences from the N2 unlabeled sentences according to a degree of consistency of determined results of the classifiers , wherein the determined results of the N3 sentences are determined by the classifiers and are inconsistent , and the N3 is a positive integer . \"\n",
        "#context = \"Each of the classifiers determines at least one corresponding sentence type of each of the N2 unlabeled sentences among the N sentences , wherein the N2 is a positive integer . The consistency evaluation module is configured to : select , in a consistency evaluation process , N3 sentences from the N2 unlabeled sentences according to a degree of consistency of determined results of the classifiers , wherein the determined results of the N3 sentences are determined by the classifiers and are inconsistent , and the N3 is a positive integer . The complementarity evaluation module is configured to : select , in a complementarity evalu ation process , N4 mutually complementary sentences as to - be - labeled sentences from the N3 sentences , wherein the N4 is a positive integer . The classifier generation module is further configured to : re - create , after the N4 selected to - be labeled sentences are labeled , the classifiers of the classifier module according to the N1 labeled sentences and the N4 selected to - be - labeled sentences . The classifier evaluation module is configured to : add at least one of previously created classifiers to the classifier module to be members of the classifier module According to an alternate embodiment of the pres ent disclosure , a computer programming product is disclosed . The computer program product is installed in a question - answering learning system to execute a question answering learning method . The question - answering learning method includes the following : \"\n",
        "#context='omplementary sentences are selected as to - be - labeled sentences from the N3 sentences by a complementarity evaluation module , wherein the words or textual meanings of the N4 sentences are not mutually repetitive , similar , implicative or derivative , and the N4 is a positive integer . After the N4 selected to - be - labeled sentences are labeled , the classifiers of the classifier module are re - created by the classifier generation module according to the N1 labeled sentences and the N4 selected to - be - labeled sentences . At least one of previously created classifiers is added to the classifier module to be members of the classifier module by a classifier evaluation module The above and other aspects of the invention will become better understood with regards to the following detailed description of the preferred but non - limiting embodiment ( s ) . The following description is made with reference to the accompanying drawings'\n",
        "context='''We present QuAC, a dataset for Question Answering in Context that contains 14K information-seeking QA dialogs (100K questions in total). The dialogs involve two crowd workers: (1) a student who poses a sequence of freeform questions to learn as much as possible about a hidden Wikipedia text, and (2) a teacher who answers the questions by providing short excerpts from the text. QuAC introduces challenges not found in existing machine comprehension datasets: its questions are often more open-ended, unanswerable, or only meaningful within the dialog context, as we show in a detailed qualitative evaluation. We also report results for a number of reference models, including a recently state-of-the-art reading comprehension architecture extended to model dialog context. Our best model underperforms humans by 20 F1, suggesting that there is significant room for future work on this data.\n",
        "Dataset, baseline, and leaderboard available at http://quac.ai '''\n",
        "#print(\"context: \",context)\n",
        "\n",
        "ques,answer = get_question1(context,model1,tokenizer1)\n",
        "print (\"question: \",ques)\n",
        "print (\"answer: \",answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sdr2KevoPiMk",
        "outputId": "34922df7-81fb-4444-b971-87ad0086f529"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "question:  What are the challenges presented in the QuAC dataset?\n",
            "answer:  The challenges encountered in existing machine comprehension datasets include open-ended, unanswerable, or meaningful within the dialog context.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context='''Our core evaluation metric, word-level F1, is implemented similarly to SQuAD : precision and recall are computed by considering the portion of words in the prediction and references that overlap after removing stopwords.12 For no answer questions, we give the system an F1 of one if it correctly predicts no answer and zero otherwise.13 Like SQuAD, we compute the maximum F1 among all references; however, since many questions have multiple valid answers, this metric varies significantly with the number of reference annotations. To make oracle human and system performance comparable, given n references, we report the average of the maximum F1 computed from each n − 1 subset with respect to the heldout reference. Additionally, since averaged F1 can be misleading for questions with multiple valid answers, we introduce the human equivalence score (HEQ), a performance measure for judging whether a system’s output is as good as that of an average human.14 HEQ measures the percentage of examples for which system F1 exceeds or matches human F1. We compute two variants: (1) the percentage of questions for which this is true (HEQ-Q), and (2) the percentage of dialogs for which this is true for every question in the dialog (HEQ-D). A system that achieves a value of 100 on HEQ-D can by definition maintain average human quality output over full dialogs. For dialog acts, we report accuracy with respect to the majority annotation, breaking ties randomly.'''\n",
        "ques,answer = get_question1(context,model1,tokenizer1)\n",
        "print (\"question: \",ques)\n",
        "print (\"answer: \",answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lvxd07vH_eHh",
        "outputId": "8ea25161-0573-4bc1-efae-1638412b870e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "question:  How does the human equivalence score (HEQ) measure the performance of a system's output as good as that of an average human?\n",
            "answer:  For questions with multiple valid answers, the metric varies significantly with the number of reference annotations. To make oracle human and system performance comparable, given\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context='''Sanity check Overall, the poor sanity check results imply that is very challenging. Of these, following the transition matrix (TM) gives the best performance, reinforcing the observation that the dialog context plays a significant role in the task. Upper bounds The human upper bound (80.8 F1) demonstrates high agreement. While Gold sentence + NA does perform well, indicating that significant progress can be made by treating the problem as answer sentence selection, HEQ measures show that span-based approaches will be needed achieve average human equivalence. Finally, the Gold NA + TM shows that cannot be solved by ignoring question and answer text Baselines Text similarity methods such as bagof-ngrams overlap and InferSent are largely ineffective on , which shows that questions have little direct overlap with their answers. On the other hand, BiDAF++ models make significant progress, demonstrating that existing models can already capture a significant portion of phenomena in . The addition of information from previous turns (w/ 1-ctx) helps significantly, indicating that integration of context is essential to solving the task. While increasing the context size in BiDAF++ continues to help, we observe saturation using contexts of length 3, suggesting that more sophisticated models are necessary to take full advantage of the context. Finally, even our best model underperforms humans:\n",
        "the system achieves human equivalence on only 60% of questions and 5% of full dialogs.'''\n",
        "ques,answer = get_question1(context,model1,tokenizer1)\n",
        "print (\"question: \",ques)\n",
        "print (\"answer: \",answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HB9xbeJLAK8c",
        "outputId": "dae9fd59-1945-4ec9-e330-b8869561c445"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "question:  What does the poor results obtained from the sanity check imply about the QuAC task?\n",
            "answer:  The lack of results from previous turns (w/ 1-ctx) helps significantly, indicating that integration of context is essential to solving the task. While increasing the context size in BiDAF++ continues to help, the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def divide_into_paragraphs(text, num_paragraphs):\n",
        "    # Split the text into paragraphs based on the desired number\n",
        "    words = text.split()\n",
        "    words_per_paragraph = len(words) // num_paragraphs\n",
        "\n",
        "    paragraphs = []\n",
        "    start_index = 0\n",
        "    for _ in range(num_paragraphs - 1):\n",
        "        end_index = start_index + words_per_paragraph\n",
        "        paragraph = \" \".join(words[start_index:end_index])\n",
        "        paragraphs.append(paragraph)\n",
        "        start_index = end_index\n",
        "\n",
        "    # Add the remaining words as the last paragraph\n",
        "    last_paragraph = \" \".join(words[start_index:])\n",
        "    paragraphs.append(last_paragraph)\n",
        "\n",
        "    return paragraphs\n"
      ],
      "metadata": {
        "id": "jDykKBK6hNDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2\n",
        "import PyPDF2\n",
        "import math\n",
        "\n",
        "def qa_func(start_page: int, end_page: int, num_qa: int):\n",
        "  #qa_dict=dict.fromkeys('Question','Answer')\n",
        "  qa_dict={}\n",
        "  num_qa_per_pg = math.ceil(num_qa/(end_page-start_page+1))\n",
        "  if (num_qa_per_pg>5): #5 is hyperparamer which means it will generate 5 questions per page\n",
        "    print('Enter less number of questions')\n",
        "  else:\n",
        "\n",
        "      pdf_file_path = \"/content/drive/MyDrive/Machine Learning - Tom Mitchell_compressed.pdf\"\n",
        "      with open(pdf_file_path, \"rb\") as file:\n",
        "          pdf_reader = PyPDF2.PdfReader(file)\n",
        "\n",
        "          # Find the start and end page numbers of the desired chapter from the index\n",
        "          # start_page = 10  # Replace with the actual start page number of the chapter\n",
        "          # end_page = 20    # Replace with the actual end page number of the chapter\n",
        "\n",
        "          # Extract pages of the desired chapter\n",
        "          extracted_pages = []\n",
        "          count=0\n",
        "          for page_num in range(start_page, end_page+1):\n",
        "              page = pdf_reader.pages[page_num].extract_text()\n",
        "              #print(page_num)\n",
        "\n",
        "              paragraphs = divide_into_paragraphs(page, num_qa_per_pg)\n",
        "              for i, paragraph in enumerate(paragraphs):\n",
        "\n",
        "                if (count<num_qa):\n",
        "                  ques,answer = get_question1(paragraph,model1,tokenizer1)\n",
        "                  print(paragraph)\n",
        "                  qa_dict[ques] = answer\n",
        "                  '''print (\"question: \",ques)\n",
        "                  print (\"answer: \",answer)'''\n",
        "                  count=count+1\n",
        "                else:\n",
        "                  #print (\"qa_dict: \",qa_dict)\n",
        "                  break\n",
        "  return qa_dict\n",
        "my_dict={}\n",
        "my_dict=qa_func(14,15,5)\n",
        "print(my_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBi21y2CiEg1",
        "outputId": "a2ee7028-8488-454e-f236-22454c79bc01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "CHAPTER 1 INTRODUCITON 3 0 Learning to recognize spoken words. All of the most successful speech recognition systems employ machine learning in some form. For example, the SPHINX system (e.g., Lee 1989) learns speaker-specific strategies for recognizing the primitive sounds (phonemes) and words from the observed speech signal. Neural network learning methods (e.g., Waibel et al. 1989) and methods for learning hidden Markov models (e.g., Lee 1989) are effective for automatically customizing to,individual speakers, vocabularies, microphone characteristics, background noise, etc. Similar techniques have potential applications in many signal-interpretation problems. 0 Learning to drive an autonomous vehicle. Machine learning methods have been used to train computer-controlled vehicles to steer correctly when driving on a variety of road types. For example, the ALVINN system (Pomerleau 1989) has used its learned strategies to drive unassisted at 70 miles per hour\n",
            "for 90 miles on public highways among other cars. Similar techniques have possible applications in many sensor-based control problems. 0 Learning to classify new astronomical structures. Machine learning methods have been applied to a variety of large databases to learn general regularities implicit in the data. For example, decision tree learning algorithms have been used by NASA to learn how to classify celestial objects from the second Palomar Observatory Sky Survey (Fayyad et al. 1995). This system is now used to automatically classify all objects in the Sky Survey, which consists of three terrabytes of image data. 0 Learning to play world-class backgammon. The most successful computer programs for playing games such as backgammon are based on machiie learning algorithms. For example, the world's top computer program for backgammon, TD-GAMMON (Tesauro 1992, 1995). learned its strategy by\n",
            "playing over one million practice games against itself. It now plays at a level competitive with the human world champion. Similar techniques have applications in many practical problems where very large search spaces must be examined efficiently. TABLE 1.1 Some successful applications of machiie learning. three features: the class of tasks, the measure of performance to be improved, and the source of experience. A checkers learning problem: Task T: playing checkers 0 Performance measure P: percent of games won against opponents Training experience E: playing practice games against itself We can specify many learning problems in this fashion, such as learning to recognize handwritten words, or learning to drive a robotic automobile au- tonomously. A handwriting recognition learning problem: 0 Task T: recognizing and classifying handwritten words within images 0 Performance measure P: percent of words correctly classified\n",
            "4 MACHINE LEARNING Artificial intelligence Learning symbolic representations of concepts. Machine learning as a search problem. Learning as an approach to improving problem solving. Using prior knowledge together with training data to guide learning. 0 Bayesian methods Bayes' theorem as the basis for calculating probabilities of hypotheses. The naive Bayes classifier. Algorithms for estimating values of unobserved variables. 0 Computational complexity theory Theoretical bounds on the inherent complexity of different learning tasks, measured in terms of the computational effort, number of training examples, number of mistakes, etc. required in order to learn. Control theory Procedures that learn to control processes in order to optimize predefined objectives and that learn to predict the next state of the process they are controlling.\n",
            "0 Information theory Measures of entropy and information content. Minimum description length approaches to learning. Optimal codes and their relationship to optimal training sequences for encoding a hypothesis. Philosophy Occam's razor, suggesting that the simplest hypothesis is the best. Analysis of the justification for generalizing beyond observed data. 0 Psychology and neurobiology The power law of practice, which states that over a very broad range of learning problems, people's response time improves with practice according to a power law. Neurobiological studies motivating artificial neural network models of learning. 0 Statistics Characterization of errors (e.g., bias and variance) that occur when estimating the accuracy of a hypothesis based on a limited sample of data. Confidence intervals, statistical tests. TABLE 1.2 Some\n",
            "{'How does the machine learning system train computer-controlled vehicles to steer correctly when driving on a variety of road types?': 'The ALVINN system (Pomerleau 1989) has used its learned strategies to drive unassisted at 70 miles per hour. It has been used to automatically customize to,individual speakers', 'What are some applications for sensor-based control problems?': '0 Learning to classify new astronomical structures. Machine learning methods have been applied to a variety of large databases to learn general regularities implicit in the data.', 'How is the handwriting recognition learning problem addressed in the TABLE 1.1 application of machiie learning?': '\"Though playing checkers 0 Performance measure P: percent of games won against opponents Training experience E: playing practice games against itself\"', 'What are the limitations of the Computational complexity theory theory?': 'The limits on the inherent complexity of different learning tasks are measured in terms of computational effort, number of training examples, and numbers of mistakes, etc. required to learn. Control theory Procedures that learn to control processes in order to optimize predefined objectives and predict the next', 'What are the characteristics of errors (e.g., bias and variance) that occur when estimating accuracy of a hypothesis based on limited sample of data?': '0 Information theory measures entropy and information content. Minimum description length approaches to learning. Optimal codes and their relationship to optimal training sequences for'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modification with Vishu's Code"
      ],
      "metadata": {
        "id": "UJd9g89hL31K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2\n",
        "import PyPDF2\n",
        "import math\n",
        "import re\n",
        "\n",
        "def qa_func(start_page: int, end_page: int, num_qa: int):\n",
        "  #qa_dict=dict.fromkeys('Question','Answer')\n",
        "  qa_dict={}\n",
        "  figure_regex = r\"\\b(Fig(?:ure)?\\.?\\s\\d+)\\b\"\n",
        "  table_regex = r\"\\b(Table\\s\\d+)\\b\"\n",
        "  num_qa_per_pg = math.ceil(num_qa/(end_page-start_page+1))\n",
        "  if (num_qa_per_pg>5): #5 is hyperparamer which means it will generate 5 questions per page\n",
        "    print('Enter less number of questions')\n",
        "  else:\n",
        "\n",
        "      pdf_file_path = \"/content/drive/MyDrive/Machine Learning - Tom Mitchell_compressed.pdf\"\n",
        "      with open(pdf_file_path, \"rb\") as file:\n",
        "          pdf_reader = PyPDF2.PdfReader(file)\n",
        "\n",
        "          # Find the start and end page numbers of the desired chapter from the index\n",
        "          # start_page = 10  # Replace with the actual start page number of the chapter\n",
        "          # end_page = 20    # Replace with the actual end page number of the chapter\n",
        "\n",
        "          # Extract pages of the desired chapter\n",
        "          extracted_pages = []\n",
        "          count=0\n",
        "          for page_num in range(start_page, end_page+1):\n",
        "              page = pdf_reader.pages[page_num].extract_text()\n",
        "              print(page_num)\n",
        "\n",
        "              # Remove figures, tables, author names, and titles\n",
        "\n",
        "              page = re.sub(figure_regex, \"\", page, flags=re.IGNORECASE)\n",
        "\n",
        "              page = re.sub(table_regex, \"\", page, flags=re.IGNORECASE)\n",
        "\n",
        "              paragraphs = divide_into_paragraphs(page, num_qa_per_pg)\n",
        "              for i, paragraph in enumerate(paragraphs):\n",
        "                #print(i)\n",
        "\n",
        "                if (count<num_qa):\n",
        "                  print(count)\n",
        "                  ques,answer = get_question1(paragraph,model1,tokenizer1)\n",
        "                  print(ques)\n",
        "                  #if(len(qa_dict) > 0):\n",
        "                  if(process_question(ques,qa_dict.keys())==\" \"):\n",
        "                    qa_dict[ques] = answer\n",
        "                    count=count+1\n",
        "                else:\n",
        "                  break\n",
        "  return qa_dict\n",
        "my_dict={}\n",
        "my_dict=qa_func(15,20,20)\n",
        "print(my_dict)"
      ],
      "metadata": {
        "id": "c0XDfA5rLHtv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question_Similarity_from Vishnu"
      ],
      "metadata": {
        "id": "5N4wTeTTLD0N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = my_dict.keys()\n",
        "print(len(x))\n",
        "for i in x:\n",
        "  print(i)\n",
        "#print(x)\n",
        "y = my_dict.items()\n",
        "print(y)#Full list\n",
        "z = my_dict.values()\n",
        "print(z)"
      ],
      "metadata": {
        "id": "fzRqKqsQIwMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class STSBertModel(torch.nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        super(STSBertModel, self).__init__()\n",
        "\n",
        "        word_embedding_model = models.Transformer('bert-base-uncased', max_seq_length=128)\n",
        "        pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
        "        self.sts_model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
        "\n",
        "    def forward(self, input_data):\n",
        "\n",
        "        output = self.sts_model(input_data)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "gXwlL27QDplW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "QopPM5c-9DHv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence_transformers\n",
        "import joblib\n",
        "trained_model = joblib.load('/content/drive/MyDrive/Copy of Sbert_Model_1')"
      ],
      "metadata": {
        "id": "qxwGOVx-9Cd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install sentence-transformers\n",
        "!pip install transformers\n",
        "!pip install torchmetrics.functional\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer, models\n",
        "from transformers import BertTokenizer\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "from datasets import load_dataset\n",
        "import matplotlib.pyplot as plt\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "G6fxUKFNP-CY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to predict test data\n",
        "def predict_sts(texts):\n",
        "\n",
        "  trained_model .to('cpu')\n",
        "  trained_model.eval()\n",
        "  test_input = tokenizer(texts, padding='max_length', max_length = 128, truncation=True, return_tensors=\"pt\")\n",
        "  test_input['input_ids'] = test_input['input_ids']\n",
        "  test_input['attention_mask'] = test_input['attention_mask']\n",
        "  del test_input['token_type_ids']\n",
        "\n",
        "  test_output = trained_model(test_input)['sentence_embedding']\n",
        "  sim = torch.nn.functional.cosine_similarity(test_output[0], test_output[1], dim=0).item()\n",
        "\n",
        "  return sim"
      ],
      "metadata": {
        "id": "GNgWF800D4ZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_question(question,questions):\n",
        "  similarity = False\n",
        "  compare_questions = []\n",
        "  similar_question = \" \"\n",
        "  if(len(question)>0):\n",
        "    for value in questions:\n",
        "      if (similarity == False):\n",
        "        compare_questions.append(question)\n",
        "        #print('Q',question)\n",
        "        compare_questions.append(value)\n",
        "        #print('V',value)\n",
        "        if predict_sts(compare_questions) > 0.8 :\n",
        "          similarity = True\n",
        "          similar_question = value\n",
        "        compare_questions.clear()\n",
        "  return similar_question\n"
      ],
      "metadata": {
        "id": "rQ9f9xu8EIQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in my_dict.keys():\n",
        "  a=process_question(i,my_dict.keys())\n",
        "  print(a)"
      ],
      "metadata": {
        "id": "d6hXpUdXFodO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in x:\n",
        "  print(i)"
      ],
      "metadata": {
        "id": "xs0j_pvlQYKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences1= [\n",
        "        \"The QuAC dataset promotes natural and diverse questions by collecting the data in an interactive setting with two crowd workers playing the roles of teacher and student. Unlike previous dialog-style QA datasets that use semi-automatic question generation, QuAC allows for genuine human interaction. This approach encourages a wider range of questions and avoids the limitations of automated question generation techniques.\",\n",
        "        \"The QuAC dataset encourages the use of natural and diverse questions by gathering information in an engaging environment with two crowd workers acting as teacher and student. QuAC enables true human involvement in contrast to earlier dialog-style QA datasets that employ semi-automatic question generating. This strategy promotes a larger variety of queries while avoiding the drawbacks of automated question generation methods.\"\n",
        "    ]"
      ],
      "metadata": {
        "id": "sZD2OzcmOYEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_sts(sentences1)"
      ],
      "metadata": {
        "id": "00vUIZLrOeBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_answer(answer,answers):\n",
        "  similar_answer = False\n",
        "  compare_answers = []\n",
        "  for value in answers:\n",
        "    if (similarity == False):\n",
        "      compare_answers.add(answer)\n",
        "      compare_answers.add(value)\n",
        "      if predict_sts(compare_answers) > 0.7:\n",
        "        similar_answer = True\n",
        "      compare_answers.clear()\n",
        "  return similar_answer"
      ],
      "metadata": {
        "id": "zIrIzpk6EXd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet gradio==3.9"
      ],
      "metadata": {
        "id": "UGbWyIRiAAtF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "context = gr.inputs.Textbox(lines=5,placeholder=\"Enter the page number upto which you want to extract question.\")\n",
        "# answer = gr.inputs.Textbox(lines=3, placeholder=\"Enter answer/keyword here...\")\n",
        "question = gr.outputs.Textbox( type=\"auto\", label=\"Question\")\n",
        "#answer = gr.outputs.Textbox( type=\"auto\", label=\"Answer\")\n",
        "\n",
        "def ques_answer(context):\n",
        "  return get_question1(context,model1,tokenizer1)\n",
        "\n",
        "iface = gr.Interface(\n",
        "  fn=ques_answer,\n",
        "  inputs=[context],\n",
        "  outputs=question)\n",
        "\n",
        "\n",
        "iface.launch(debug=False,share=True)"
      ],
      "metadata": {
        "id": "5Xxt1BbY8n80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out = []\n",
        "threshold = int(len(text)/4)\n",
        "#print(threshold)\n",
        "for chunk in text.split('. '):\n",
        "    if out and len(chunk)+len(out[-1]) < threshold:\n",
        "        out[-1] += ' '+chunk+'.'\n",
        "        print(out)\n",
        "    else:\n",
        "        #print(out)\n",
        "        out.append(chunk+'.')\n",
        "    print(out)"
      ],
      "metadata": {
        "id": "ZDSc3o7LqELu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r4rPN611qgkd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text='According to one embodiment of the present dis closure , a question - answering learning method is provided . The question - answering learning method includes the following . A classifier module is created by a classifier gen eration module according to N1 labeled sentences among N sentences , wherein the classifier module includes several classifiers each representing different question - answering models , and both of the N and the N1 are positive integers . At least one corresponding sentence type of each of the N2 unlabeled sentences among the N sentences is determined by each of the classifiers , wherein the N2 is a positive integer . In a consistency evaluation process , N3 sentences are selected from the N2 unlabeled sentences by a consistency evaluation module according to a degree of consistency of determined results of the classifiers , wherein the determined results of the N3 sentences are determined by the classifiers and are inconsistent , and the N3 is a positive integer . In a complementarity evaluation process , N4 mutually complementary sentences are selected as to - be - labeled sentences from the N3 sentences by a complementarity evaluation module , wherein the N4 is a positive integer . After the N4 selected to - be - labeled sentences are labeled , the classifiers of the classifier module are re - created by the classifier generation module according to the N1 labeled sentences and the N4 selected to - be - labeled sentences . At least one of previously created classifiers is added to the classifier module to be members of the classifier module by a classifier evaluation module. According to another embodiment of the present disclosure , a question - answering learning system is provided . The question - answering learning system includes a classifier generation module , a consistency evaluation module , a complementarity evaluation module and a classifier evaluation module . The classifier generation module is configured to : create a classifier module according to N1 labeled sentences among N sentences , wherein the classifier module includes several classifiers each representing different ques tion - answering models , and both of the N and the N1 are positive integers . Each of the classifiers determines at least one corresponding sentence type of each of the N2 unlabeled sentences among the N sentences , wherein the N2 is a positive integer . The consistency evaluation module is configured to : select , in a consistency evaluation process , N3 sentences from the N2 unlabeled sentences according to a degree of consistency of determined results of the classifiers , wherein the determined results of the N3 sentences are determined by the classifiers and are inconsistent , and the N3 is a positive integer . The complementarity evaluation module is configured to : select , in a complementarity evalu ation process , N4 mutually complementary sentences as to - be - labeled sentences from the N3 sentences , wherein the N4 is a positive integer . The classifier generation module is further configured to : re - create , after the N4 selected to - be labeled sentences are labeled , the classifiers of the classifier module according to the N1 labeled sentences and the N4 selected to - be - labeled sentences . The classifier evaluation module is configured to : add at least one of previously created classifiers to the classifier module to be members of the classifier module According to an alternate embodiment of the pres ent disclosure , a computer programming product is disclosed . The computer program product is installed in a question - answering learning system to execute a question answering learning method . The question - answering learning method includes the following : A classifier module is created by a classifier generation module according to N1 labeled sentences among N sentences , wherein the classifier module includes several classifiers each representing different question - answering models , and both of the N and the N1 are positive integers . At least one corresponding sentence type of each of the N2 unlabeled sentences among the N sentences is determined by each of the classifiers , wherein the N2 is a positive integer . In a consistency evaluation process , N3 sentences are selected from the N2 unlabeled sentences by a consistency evaluation module according to a degree of consistency of determined results of the classifiers , wherein the determined results of the N3 sentences are determined by the classifiers and are inconsistent ,and the N3 is a positive integer . In a complementarity evaluation process , N4 mutually complementary sentences are selected as to - be - labeled sentences from the N3 sentences by a complementarity evaluation module , wherein the words or textual meanings of the N4 sentences are not mutually repetitive , similar , implicative or derivative , and the N4 is a positive integer . After the N4 selected to - be - labeled sentences are labeled , the classifiers of the classifier module are re - created by the classifier generation module according to the N1 labeled sentences and the N4 selected to - be - labeled sentences . At least one of previously created classifiers is added to the classifier module to be members of the classifier module by a classifier evaluation module The above and other aspects of the invention will become better understood with regards to the following detailed description of the preferred but non - limiting embodiment ( s ) . The following description is made with reference to the accompanying drawings '"
      ],
      "metadata": {
        "id": "5TioWQw8qHl9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ClusQuesGen(a,b,n):\n",
        "#a=1st page of the book, b=last page of the pdf, n=number of questions you want from each page\n",
        "      import PyPDF2\n",
        "      from sentence_transformers import SentenceTransformer\n",
        "      from sklearn.cluster import KMeans\n",
        "\n",
        "      embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "      reader = PyPDF2.PdfReader('/content/drive/MyDrive/2006.09595 (1).pdf')\n",
        "\n",
        "      # Corpus with example sentences\n",
        "      #corpus = ['A man is eating food. A man is eating a piece of bread. A man is eating pasta.The girl is carrying a baby. The baby is carried by the womanA man is riding a horse. A man is riding a white horse on an enclosed ground. A monkey is playing drumsSomeone in a gorilla costume is playing a set of drums. A cheetah is running behind its prey. A cheetah chases prey on across a field.']\n",
        "      #for pg in range(0,len(reader.pages)):\n",
        "      ques_answer=[]\n",
        "      for pg in range(a,b):\n",
        "        #print(\"New Para: \"+ str(pg))\n",
        "          context=reader.pages[pg].extract_text()\n",
        "          corpus1 = context.split('. ')\n",
        "          #print(corpus1)\n",
        "          corpus_embeddings = embedder.encode(corpus1)\n",
        "          num_clusters = n\n",
        "          clustering_model = KMeans(n_clusters=num_clusters)\n",
        "          clustering_model.fit(corpus_embeddings)\n",
        "          cluster_assignment = clustering_model.labels_\n",
        "          #print(cluster_assignment)\n",
        "          clustered_sentences = [[] for i in range(num_clusters)]\n",
        "          for sentence_id, cluster_id in enumerate(cluster_assignment):\n",
        "            clustered_sentences[cluster_id].append(corpus1[sentence_id])\n",
        "            #print(clustered_sentences)\n",
        "          for i, cluster in enumerate(clustered_sentences):\n",
        "            print(\"Context \", i+1)\n",
        "            print(cluster)\n",
        "            print(\"\")\n",
        "            ques, answer = get_question1(cluster,model1,tokenizer1)\n",
        "            ques_answer.append([cluster, i+1, ques, answer])\n",
        "            #return cluster, ques, answer\n",
        "            #print(ques_answer)\n",
        "            print (\"question: \",ques)\n",
        "            print (\"answer: \",answer)\n",
        "      #return cluster, ques, answer\n"
      ],
      "metadata": {
        "id": "3OOp-uR-6lFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence_transformers\n",
        "ClusQuesGen(2,3,1)"
      ],
      "metadata": {
        "id": "_dR5QgeP6Jsi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "T5 Large on Race"
      ],
      "metadata": {
        "id": "a-6grY1nEpGY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"potsawee/t5-large-generation-squad-QuestionAnswer\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"potsawee/t5-large-generation-squad-QuestionAnswer\")\n",
        "\n",
        "\n",
        "tokenizer1 = AutoTokenizer.from_pretrained(\"potsawee/t5-large-generation-race-QuestionAnswer\")\n",
        "model1 = AutoModelForSeq2SeqLM.from_pretrained(\"potsawee/t5-large-generation-race-QuestionAnswer\")\n",
        "\n",
        "context = r\"\"\"The Sino-Indian War also known as the China–India War, Indo-China War, Indo-China War of 1962 or Sino-Indian War of 1962, was a military conflict between China and India that took place from October to November 1962. It was a military escalation of the Sino-Indian border dispute. Fighting occurred along India's border with China, in India's North-East Frontier Agency east of Bhutan, and in Aksai Chin west of Nepal.\"\"\".replace('\\n', ' ')\n",
        "#context = a\n",
        "for i in context:\n",
        "      #print(context[i])\n",
        "\n",
        "      inputs = tokenizer(context[i], return_tensors=\"pt\",truncation=True,padding=True)\n",
        "      outputs = model.generate(**inputs, max_length=100)\n",
        "      question_answer = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
        "      question_answer = question_answer.replace(tokenizer.pad_token, \"\").replace(tokenizer.eos_token, \"\")\n",
        "      question, answer = question_answer.split(tokenizer.sep_token)\n",
        "      print(\"extractive question:\", question)\n",
        "      print(\"extractive answer:\", answer)\n",
        "\n",
        "      inputs = tokenizer1(context[i], return_tensors=\"pt\",truncation=True,padding=True)\n",
        "      outputs = model1.generate(**inputs, max_length=100)\n",
        "      question_answer = tokenizer1.decode(outputs[0], skip_special_tokens=False)\n",
        "      question_answer = question_answer.replace(tokenizer1.pad_token, \"\").replace(tokenizer1.eos_token, \"\")\n",
        "      question, answer = question_answer.split(tokenizer1.sep_token)\n",
        "      print(\"abstractive question:\", question)\n",
        "      print(\"abstractive answer:\", answer)"
      ],
      "metadata": {
        "id": "IlRkixQtEsOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t5('''The Sino-Indian War also known as the China–India War, Indo-China War, Indo-China War of 1962 or Sino-Indian War of 1962, was a military conflict between China and India that took place from October to November 1962. It was a military escalation of the Sino-Indian border dispute. Fighting occurred along India's border with China, in India's North-East Frontier Agency east of Bhutan, and in Aksai Chin west of Nepal.''')"
      ],
      "metadata": {
        "id": "_uPdyrBZGi5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "snTA4ysrHWTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "T5 Base on Race"
      ],
      "metadata": {
        "id": "uDEurh_aEtNZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import glob\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import logging\n",
        "import random\n",
        "from itertools import chain\n",
        "from string import punctuation\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from transformers import (\n",
        "    AdamW,\n",
        "    T5ForConditionalGeneration,\n",
        "    T5Tokenizer,\n",
        "    get_linear_schedule_with_warmup\n",
        ")\n",
        "class QueGenerator():\n",
        "  def __init__(self):\n",
        "    self.que_model = T5ForConditionalGeneration.from_pretrained('./t5_que_gen_model/t5_base_que_gen/')\n",
        "    self.ans_model = T5ForConditionalGeneration.from_pretrained('./t5_ans_gen_model/t5_base_ans_gen/')\n",
        "\n",
        "    self.que_tokenizer = T5Tokenizer.from_pretrained('./t5_que_gen_model/t5_base_tok_que_gen/')\n",
        "    self.ans_tokenizer = T5Tokenizer.from_pretrained('./t5_ans_gen_model/t5_base_tok_ans_gen/')\n",
        "\n",
        "    self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    self.que_model = self.que_model.to(self.device)\n",
        "    self.ans_model = self.ans_model.to(self.device)\n",
        "\n",
        "  def generate(self, text):\n",
        "        answers = self._get_answers(text)\n",
        "        questions = self._get_questions(text, answers)\n",
        "        output = [{'answer': ans, 'question': que} for ans, que in zip(answers, questions)]\n",
        "        return output\n",
        "\n",
        "  def _get_answers(self, text):\n",
        "    # split into sentences\n",
        "    sents = sent_tokenize(text)\n",
        "\n",
        "    examples = []\n",
        "    for i in range(len(sents)):\n",
        "      input_ = \"\"\n",
        "      for j, sent in enumerate(sents):\n",
        "        if i == j:\n",
        "            sent = \"[HL] %s [HL]\" % sent\n",
        "        input_ = \"%s %s\" % (input_, sent)\n",
        "        input_ = input_.strip()\n",
        "      input_ = input_ + \" </s>\"\n",
        "      examples.append(input_)\n",
        "\n",
        "    batch = self.ans_tokenizer.batch_encode_plus(examples, max_length=512, pad_to_max_length=True, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "      outs = self.ans_model.generate(input_ids=batch['input_ids'].to(self.device),\n",
        "                                attention_mask=batch['attention_mask'].to(self.device),\n",
        "                                max_length=32,\n",
        "                                # do_sample=False,\n",
        "                                # num_beams = 4,\n",
        "                                )\n",
        "    dec = [self.ans_tokenizer.decode(ids, skip_special_tokens=False) for ids in outs]\n",
        "    answers = [item.split('[SEP]') for item in dec]\n",
        "    answers = chain(*answers)\n",
        "    answers = [ans.strip() for ans in answers if ans != ' ']\n",
        "    return answers\n",
        "\n",
        "  def _get_questions(self, text, answers):\n",
        "    examples = []\n",
        "    for ans in answers:\n",
        "      input_text = \"%s [SEP] %s </s>\" % (ans, text)\n",
        "      examples.append(input_text)\n",
        "\n",
        "    batch = self.que_tokenizer.batch_encode_plus(examples, max_length=512, pad_to_max_length=True, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "      outs = self.que_model.generate(input_ids=batch['input_ids'].to(self.device),\n",
        "                                attention_mask=batch['attention_mask'].to(self.device),\n",
        "                                max_length=32,\n",
        "                                num_beams = 4)\n",
        "    dec = [self.que_tokenizer.decode(ids, skip_special_tokens=False) for ids in outs]\n",
        "    return dec\n",
        "\n",
        "que_generator = QueGenerator()\n",
        "\n",
        "text = \"Python is an interpreted, high-level, general-purpose programming language. Created by Guido van Rossum \\\n",
        "  and first released in 1991, Python's design philosophy emphasizes code \\\n",
        "  readability with its notable use of significant whitespace.\"\n",
        "\n",
        "text2 = \"Gravity (from Latin gravitas, meaning 'weight'), or gravitation, is a natural phenomenon by which all \\\n",
        "  things with mass or energy—including planets, stars, galaxies, and even light—are brought toward (or gravitate toward) \\\n",
        "  one another. On Earth, gravity gives weight to physical objects, and the Moon's gravity causes the ocean tides. \\\n",
        "  The gravitational attraction of the original gaseous matter present in the Universe caused it to begin coalescing \\\n",
        "  and forming stars and caused the stars to group together into galaxies, so gravity is responsible for many of \\\n",
        "  the large-scale structures in the Universe. Gravity has an infinite range, although its effects become increasingly \\\n",
        "  weaker as objects get further away\"\n",
        "print(que_generator.generate(text2))"
      ],
      "metadata": {
        "id": "uSE3TejY2rrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bert"
      ],
      "metadata": {
        "id": "9GHR8gP9FqAC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertForQuestionAnswering\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "#Model\n",
        "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "\n",
        "#Tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "question = '''What Nikhil finds?'''\n",
        "\n",
        "paragraph = ''' 10 years earlier, Nikhil finds out that DJ had framed the evidence to send Shubh to prison, which is the reason why Nikhil resigned from the CBI. At present,\n",
        "Nikhil is assigned DJ as the next victim and tells the killer that he has to go to jail to kill him since the central jail has high surveillance. The killer gets into the jail and as plotted,\n",
        " reaches DJ and is about to kill him'''\n",
        "\n",
        "encoding = tokenizer.encode_plus(text=question, text_pair=paragraph)\n",
        "\n",
        "inputs = encoding['input_ids']  # Token embeddings\n",
        "sentence_embedding = encoding['token_type_ids']  # Segment embeddings\n",
        "tokens = tokenizer.convert_ids_to_tokens(inputs)  # input tokens\n",
        "start_scores, end_scores = model(input_ids=torch.tensor([inputs]), token_type_ids=torch.tensor([sentence_embedding]),return_dict=False)\n",
        "start_index = torch.argmax(start_scores)\n",
        "\n",
        "end_index = torch.argmax(end_scores)\n",
        "\n",
        "answer = ' '.join(tokens[start_index:end_index+1])\n",
        "corrected_answer = ''\n",
        "\n",
        "for word in answer.split():\n",
        "\n",
        "    # If it's a subword token\n",
        "    if word[0:2] == '##':\n",
        "        corrected_answer += word[2:]\n",
        "    else:\n",
        "        corrected_answer += ' ' + word\n",
        "\n",
        "print(question)\n",
        "print(corrected_answer)"
      ],
      "metadata": {
        "id": "xr197-ZIGM6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchmetrics.functional import precision_recall\n",
        "preds  = torch.tensor([2, 0, 2, 1])\n",
        "target = torch.tensor([1, 1, 2, 0])\n",
        "precision_recall(preds, target, average='macro', num_classes=3)\n",
        "\n",
        "precision_recall(preds, target, average='micro')\n"
      ],
      "metadata": {
        "id": "CIFarKr_JTro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchmetrics import F1Score\n",
        "target = torch.tensor([0, 1, 2, 0, 1, 2])\n",
        "preds = torch.tensor([0, 2, 1, 0, 0, 1])\n",
        "f1 = F1Score(num_classes=3)\n",
        "f1(preds, target)"
      ],
      "metadata": {
        "id": "3PnlhQT8cIep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install anvil-uplink"
      ],
      "metadata": {
        "id": "dWlZZ8WMKdvX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import anvil.server\n",
        "anvil.server.connect(\"server_E3FCTYIOKPCC246OZC7ZI3IS-NOPF6GOREISCK3NF\")\n",
        "@anvil.server.callable\n",
        "def value_check(file_data, N):\n",
        "  print(file_data)\n",
        "  print(N)\n",
        "anvil.server.wait_forever()\n"
      ],
      "metadata": {
        "id": "rFhuBO2sKeuT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sent2vec"
      ],
      "metadata": {
        "id": "owrg9K_ZoOUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyspark"
      ],
      "metadata": {
        "id": "hb0_r-Yt8b7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession"
      ],
      "metadata": {
        "id": "KDAwLSG3tgwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder \\\n",
        "      .master(\"local[1]\") \\\n",
        "      .appName(\"SparkByExamples.com\") \\\n",
        "      .getOrCreate()\n"
      ],
      "metadata": {
        "id": "A6cgqxjJwYte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataList = [(\"Java\", 20000), (\"Python\", 100000), (\"Scala\", 3000)]\n",
        "rdd=spark.sparkContext.parallelize(dataList)\n",
        "rdd2 = spark.sparkContext.textFile(\"/path/test.txt\")"
      ],
      "metadata": {
        "id": "jtxnYHH2wht7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data1 = [('James','','Smith','1991-04-01','M',3000),\n",
        "  ('Michael','Rose','','2000-05-19','M',4000),\n",
        "  ('Robert','','Williams','1978-09-05','M',4000),\n",
        "  ('Maria','Anne','Jones','1967-12-01','F',4000),\n",
        "  ('Jen','Mary','Brown','1980-02-17','F',-1)\n",
        "]\n",
        "\n",
        "columns = [\"firstname\",\"middlename\",\"lastname\",\"dob\",\"gender\",\"salary\"]\n",
        "df = spark.createDataFrame(data=data1, schema = columns)\n"
      ],
      "metadata": {
        "id": "oINf7xLmw3im"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in df:\n",
        "  print(i)"
      ],
      "metadata": {
        "id": "U7K8vlhQxeiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()\n",
        "print(type(df))"
      ],
      "metadata": {
        "id": "ula7EiEAx2KG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns = [\"firstname\",\"middlename\",\"lastname\",\"dob\",\"gender\",\"salary\"]\n",
        "df = spark.createDataFrame(data=data, schema = columns)"
      ],
      "metadata": {
        "id": "0W4OUZUnxqDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x=lambda a:a+a\n",
        "print(x(4))"
      ],
      "metadata": {
        "id": "ktuHuvx8z-iL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "unbwBTuj1BKC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d5e11c3b5fff46eeaf2c85993ace75b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_74a8a44aedf343f2a98fdd00757853d7",
              "IPY_MODEL_c19ac1653d26498bbb9d6ef19c607f46",
              "IPY_MODEL_83cf44b7e2794d36aac3cf91eb2b0b0a"
            ],
            "layout": "IPY_MODEL_201709eb32744efea1a008ffe586bdc2"
          }
        },
        "74a8a44aedf343f2a98fdd00757853d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6168d397b8d8421eb4dde74d4cc0edfe",
            "placeholder": "​",
            "style": "IPY_MODEL_2c8bb827a78d45ed94a1f4473dbfea25",
            "value": "Downloading (…)ve/main/spiece.model: 100%"
          }
        },
        "c19ac1653d26498bbb9d6ef19c607f46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf126876f4964cf4afb22b9b1761da37",
            "max": 791656,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ee042054b6c640d792a9a41b78de2046",
            "value": 791656
          }
        },
        "83cf44b7e2794d36aac3cf91eb2b0b0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2a446e8c9c954c94a626235260066dc2",
            "placeholder": "​",
            "style": "IPY_MODEL_24f399cc816a4c2e950fc7412263cb12",
            "value": " 792k/792k [00:00&lt;00:00, 3.75MB/s]"
          }
        },
        "201709eb32744efea1a008ffe586bdc2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6168d397b8d8421eb4dde74d4cc0edfe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c8bb827a78d45ed94a1f4473dbfea25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bf126876f4964cf4afb22b9b1761da37": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee042054b6c640d792a9a41b78de2046": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2a446e8c9c954c94a626235260066dc2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24f399cc816a4c2e950fc7412263cb12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "977c6b81dea447cba92c8447a8f9c145": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cf784c8b0e184a6d868ac799235f9912",
              "IPY_MODEL_461f05816fb3479e95361b1ebf5070b9",
              "IPY_MODEL_c5e5bfeae6d14e01a4d77f664e5acd2c"
            ],
            "layout": "IPY_MODEL_96f60849927d481888a26b8a01c8da2e"
          }
        },
        "cf784c8b0e184a6d868ac799235f9912": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_22ea660ee0ac4158844292a259c02dd7",
            "placeholder": "​",
            "style": "IPY_MODEL_9c083418876b416095f9a1d6b35942a0",
            "value": "Downloading (…)okenizer_config.json: 100%"
          }
        },
        "461f05816fb3479e95361b1ebf5070b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_94eb96b24434400e81ba9bb86fe48d7d",
            "max": 2324,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_eff6419b09aa489ebd3c9593338d1435",
            "value": 2324
          }
        },
        "c5e5bfeae6d14e01a4d77f664e5acd2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_da0f7b3bed744817971f085b939b79ee",
            "placeholder": "​",
            "style": "IPY_MODEL_e298678b21144c35a26544d9e96bfd19",
            "value": " 2.32k/2.32k [00:00&lt;00:00, 133kB/s]"
          }
        },
        "96f60849927d481888a26b8a01c8da2e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22ea660ee0ac4158844292a259c02dd7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c083418876b416095f9a1d6b35942a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "94eb96b24434400e81ba9bb86fe48d7d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eff6419b09aa489ebd3c9593338d1435": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "da0f7b3bed744817971f085b939b79ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e298678b21144c35a26544d9e96bfd19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5a7550bcc87746e08816d7eaf3bd041d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_31ad55285c0c4a7ca2c0a215a23fb63e",
              "IPY_MODEL_0170d9b4cc4a4afbb6488b3502a24b31",
              "IPY_MODEL_b9f6b58de4884fdf8cf7f2de033836af"
            ],
            "layout": "IPY_MODEL_28e9f0a52db84aec8d67a75791e5a055"
          }
        },
        "31ad55285c0c4a7ca2c0a215a23fb63e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_315a6cdfcaa949f1830c6693811979e1",
            "placeholder": "​",
            "style": "IPY_MODEL_5c014b3ef0df4738abd9f911fb26d4b0",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "0170d9b4cc4a4afbb6488b3502a24b31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cf400b5c4d3049fea8a31d1beedbd2a9",
            "max": 1206,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_58e7fa13a64f457d87b3dc0b3dd65eb7",
            "value": 1206
          }
        },
        "b9f6b58de4884fdf8cf7f2de033836af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d640382e5b2e4d7d8b13507286a1e969",
            "placeholder": "​",
            "style": "IPY_MODEL_527b3e74dc5c45cbb162097d359f6c42",
            "value": " 1.21k/1.21k [00:00&lt;00:00, 50.6kB/s]"
          }
        },
        "28e9f0a52db84aec8d67a75791e5a055": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "315a6cdfcaa949f1830c6693811979e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c014b3ef0df4738abd9f911fb26d4b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cf400b5c4d3049fea8a31d1beedbd2a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58e7fa13a64f457d87b3dc0b3dd65eb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d640382e5b2e4d7d8b13507286a1e969": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "527b3e74dc5c45cbb162097d359f6c42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d0497d0eb7154813a42de4d980369ef0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_198ab701b24542c68001a66196749e4c",
              "IPY_MODEL_eed1f48b63ff4c71a4dbdebf6c06cfca",
              "IPY_MODEL_3b3f1e1986404d52a493cdf8eb6bd013"
            ],
            "layout": "IPY_MODEL_1b72316f5ca44e119d48a1041d1ec3e2"
          }
        },
        "198ab701b24542c68001a66196749e4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_30310898682a4be2ad229c97a3e6da33",
            "placeholder": "​",
            "style": "IPY_MODEL_fe219ceea3994a2eae493aa50eb0c2ae",
            "value": "Downloading model.safetensors: 100%"
          }
        },
        "eed1f48b63ff4c71a4dbdebf6c06cfca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d94ed8dadd4b4824a37b5565c7b4e82d",
            "max": 242043056,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_24453c487d994db9a878f3be44eef935",
            "value": 242043056
          }
        },
        "3b3f1e1986404d52a493cdf8eb6bd013": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e31a1e2cd1ec4c35b04fb750bd9b9ea0",
            "placeholder": "​",
            "style": "IPY_MODEL_068d7db575f246d1b2b611b63cb6aaf2",
            "value": " 242M/242M [00:00&lt;00:00, 400MB/s]"
          }
        },
        "1b72316f5ca44e119d48a1041d1ec3e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30310898682a4be2ad229c97a3e6da33": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe219ceea3994a2eae493aa50eb0c2ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d94ed8dadd4b4824a37b5565c7b4e82d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24453c487d994db9a878f3be44eef935": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e31a1e2cd1ec4c35b04fb750bd9b9ea0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "068d7db575f246d1b2b611b63cb6aaf2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "41ad0b3d44154c8fb73e5618e20cd49a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e4dfb0488eb44b8a8de06f2cb573a7ac",
              "IPY_MODEL_e06ecac16c3c4ed19005199adc6e6900",
              "IPY_MODEL_ff8001222b314654bb9f276de4386a80"
            ],
            "layout": "IPY_MODEL_386b7b0c0b8a427c95b359552fe9db9e"
          }
        },
        "e4dfb0488eb44b8a8de06f2cb573a7ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc35f104d9da47ecb4165532c97785a9",
            "placeholder": "​",
            "style": "IPY_MODEL_8e81cb8ffc9349c69dafcc15f5cd96a8",
            "value": "Downloading (…)neration_config.json: 100%"
          }
        },
        "e06ecac16c3c4ed19005199adc6e6900": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_79df26fa0fe247b896975d76988406dd",
            "max": 147,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_80f0d71a22644d00a78a3a0e529835ec",
            "value": 147
          }
        },
        "ff8001222b314654bb9f276de4386a80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_124ab81661594413a33461cc88d745e8",
            "placeholder": "​",
            "style": "IPY_MODEL_b2c385ee06e54dca873dd64168cefffb",
            "value": " 147/147 [00:00&lt;00:00, 7.36kB/s]"
          }
        },
        "386b7b0c0b8a427c95b359552fe9db9e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc35f104d9da47ecb4165532c97785a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e81cb8ffc9349c69dafcc15f5cd96a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "79df26fa0fe247b896975d76988406dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "80f0d71a22644d00a78a3a0e529835ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "124ab81661594413a33461cc88d745e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b2c385ee06e54dca873dd64168cefffb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}